{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"top\"></a>\n",
    "# QKeras-Mod Explained\n",
    "Author: Luca Urbinati, PhD Student @ Politecnico di Torino, luca.urbinati@polito.it. Date: 20/01/2024, v.1.0\n",
    "***\n",
    "\n",
    "### Content of this notebook\n",
    "[Chapter 1](#ch1): how to design a quantized model (with and without fused batch normalization) starting from a Keras model using a <b>modified version of QKeras<b> [1] that <b>quantizes weights and activations to integers<b> implementing uniform integer quantization;\n",
    "\n",
    "[Chapter 2](#ch2): <b>compare inference results</b> between the Keras model and the quantized one;\n",
    "\n",
    "[Chapter 3](#ch3): how to <b>extract quantization factors</b> (scaling factors and zero points) from each layer of the QKeras model to behave similarly to Tensorflow Lite [2][3];\n",
    "\n",
    "[Chapter 4](#ch4): how to use <b>AutoQKeras</b> to search for the best mixed-precision integer quantized model.\n",
    "\n",
    "***\n",
    "    \n",
    "### Requirements before to start\n",
    "- Read [this QKeras tutorial](https://github.com/google/qkeras/blob/master/notebook/QKerasTutorial.ipynb) to become confident with QKeras.\n",
    "- Install the conda environment [qkeras-env.yml](https://github.com/LucaUrbinati44/qkeras-mod/blob/main/qkeras-env.yml) provided in this repo and activate it (_conda activate qkeras-env_).\n",
    "- Apply the patch to QKeras' installation to have access to the modified version of QKeras (see the [README](https://github.com/LucaUrbinati44/qkeras-mod/blob/main/README.md)). \n",
    "\n",
    "### Publications using this code\n",
    "- Luca Urbinati and Mario R. Casu, \"High-Level Design of Precision-Scalable DNN Accelerators Based on Sum-Together Multiplier\", in the review process.\n",
    "\n",
    "### References\n",
    "[1] QKeras: https://github.com/google/qkeras\n",
    "\n",
    "[2] B. Jacob et al., \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,\" arXiv:1712.05877 [cs, stat], Dec. 2017. Available: http://arxiv.org/abs/1712.05877\n",
    "\n",
    "[3] Mao, Lei. \"Quantization for Neural Networks\". Lei Mao’s Log Book, May 17, 2020, https://leimao.github.io/article/Neural-Networks-Quantization/\n",
    "\n",
    "[4] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van Baalen, and T. Blankevoort, “A White Paper on Neural Network Quantization.” arXiv, Jun. 15, 2021. Available: http://arxiv.org/abs/2106.08295\n",
    "\n",
    "[5] H. Wu, P. Judd, X. Zhang, M. Isaev, and P. Micikevicius, “Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation,” arXiv:2004.09602 [cs, stat], Apr. 2020, Accessed: Dec. 22, 2021. [Online]. Available: http://arxiv.org/abs/2004.09602."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class=\"anchor\" id=\"ch0\"></a>\n",
    "# 0) Import libraries and data\n",
    "\n",
    "Go to next: [Ch. 1](#ch1).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 23:36:00.205681: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from IPython.utils import io\n",
    "import math\n",
    "from copy import deepcopy as dc\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from qkeras import *\n",
    "from qkeras.utils import *\n",
    "from qkeras.autoqkeras import *\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.keras.backend.floatx()\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize, precision=128, suppress=True)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU') == []:\n",
    "    print(\"No GPU available\")\n",
    "else:\n",
    "    print(\"GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def get_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape + (1,)).astype(\"float32\")\n",
    "    x_test = x_test.reshape(x_test.shape + (1,)).astype(\"float32\")\n",
    "\n",
    "    x_train /= 256.0\n",
    "    x_test /= 256.0\n",
    "\n",
    "    x_mean = np.mean(x_train, axis=0)\n",
    "\n",
    "    x_train -= x_mean\n",
    "    x_test -= x_mean\n",
    "\n",
    "    nb_classes = np.max(y_train)+1\n",
    "    y_train = to_categorical(y_train, nb_classes)\n",
    "    y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "input_width = 28\n",
    "input_channels = 1\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = get_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class=\"anchor\" id=\"ch1\"></a>\n",
    "# 1) Quantized network design\n",
    "\n",
    "Go to next: [Ch. 2](#ch2).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top).\n",
    "\n",
    "The goal of this chapter is to design a Convolutional network that provides quantized inputs and weights to its 2D-Conv kernels, as an example.\n",
    "\n",
    "It is important to know that inside the quantized kernels of QKeras (```QConv2D```, ```QDepthwiseConv2D```, ```QDense```) there are the corresponding TensorFlow kernels (```tf.keras.backend.conv2d()```, ```tf.keras.backend.depthwise_conv2d()```, ```tf.keras.backend.dot()```) which are floating-point kernels. \n",
    "\n",
    "When running one of these quantized kernels, QKeras partially uses the technique called [\"fake quantization\"](https://github.com/google/qkeras/issues/96#issuecomment-1210877800) that is the same technique used by Tensorflow Lite [2]. This technique consists in quantizing and dequantizing inputs and weights before running the floating-point kernel. In this way, inputs, weights (and then outputs) remain floating point numbers, but can represent quantized values only. However, there is a difference: QKeras does not fake-quantize the inputs, i.e. they remain \"true\" floating point numbers so they can represent any number in the floating point range (you can look at the source code of ```QConv2D()``` in [qconvolutional.py#L294](https://github.com/google/qkeras/blob/eb6e0dc86c43128c6708988d9cb54d1e106685a4/qkeras/qconvolutional.py#L294) yourself). The same holds also for the outputs: they remain in floating point because computing a kernel with floating-point inputs and fake-quantized weights gives floating-point outputs.\n",
    "\n",
    "To tackle this problem, we perform a ```quantized_bits()``` operation on the input feature map tensor, by inserting a ```QActivation``` layer. ```quantized_bits()``` performs a quantization-dequantization (q-deq) operation on the floating point tensor. Thanks to this ```QActivation``` layer, we can extract the quantization parameters of the input feature map tensor and quantize it to integer values (see [Chapter 2](#ch2)). Now the output tensor of ```QConv2D``` is fake-quantized completely because both inputs and weights to this layer are fake-quantized.\n",
    "\n",
    "The next two layers are a standard ```ReLU``` followed by another ```QActivation``` with ```quantized_bits()```. We could have used ```QActivation(\"quantized_relu(bits,integer)\")```, but ```quantized_relu()``` does not quantize the input data in the same way as ```quantized_bits()```. In particular, the argument ```alpha=\"auto\"``` is not present in ```quantized_relu()```, so it does not quantize with the standard affine quantization mapping formula [2][3] (shown below) which is the quantization we want to implement.\n",
    "$$x_q = \\text{clip}\\Big( \\text{round}\\big(\\frac{1}{s} x + z\\big), \\alpha_q, \\beta_q \\Big)$$\n",
    "Thus, in order to tell QKeras to use this quantization mapping formula, we have to set ```alpha=\"auto\"``` in ```quantized_bits()``` for all QKeras layers.\n",
    "\n",
    "The only drawback of using ```quantized_bits()``` is that it implements only a symmetric quantized range when ```alpha=\"auto\"``` (as written in the comment [quantizers.py#L1404](https://github.com/google/qkeras/blob/c5051b51ac5d8db7b5d235419a1538258a35a8a7/qkeras/quantizers.py#L1404)), so even if we set ```symmetric=0``` and ```keep_negative=0```, it automatically forces ```symmetric=1``` ([quantizers.py#L524](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L524)) and ```keep_negative=1``` ([quantizers.py#L584](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L524), [quantizers.py#L603](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L524)). Thefore, as an example, ```quantized_bits(4,4,0,0,alpha='auto')``` will be treated by QKeras as ```quantized_bits(4,4,1,1,alpha='auto')```. \n",
    "This implies that our features will lose 1 bit in the positive range after passing a ReLU activation.\n",
    "\n",
    "Finally, the ```QActivation``` layer that follows the ```ReLU``` can be used to fake-quantize the features to another bitwidth precision. ```ReLU``` can NOT be passed to ```QConv2D``` in the ```activation``` argument because it would be threated as ```quantized_relu()```.\n",
    "\n",
    "Regarding the number of bits for ```quantized_bits()```, we want that ```bits``` = ```integer``` because our target is to implement integer-only arithmetic. \n",
    "\n",
    "<br><br>\n",
    "STRANGE THINGS.\n",
    "1) Using ```bits``` > 31 quantizes things with ```nan```. Why? Future work\n",
    "\n",
    "2) Always explicit the value of the keyword argument ```alpha``` of  ```quantized_bits()```, that is never leave the field blank, to avoid [strange behaviors](https://github.com/google/qkeras/issues/60)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User settings\n",
    "\n",
    "# BATCHNORM PARAMS\n",
    "fused_batchnorm = 1\n",
    "\n",
    "# POOL PARAMS\n",
    "pool_size_list = [(4, 4)]\n",
    "\n",
    "# 2DCONV PARAMS\n",
    "filters_list = [2, 3]\n",
    "kernel_size_list = [(3, 3), (3, 3)]\n",
    "strides_list = [(1, 1), (2, 2)]\n",
    "pads_list = [\"valid\", \"same\"]\n",
    "\n",
    "# DENSE PARAMS\n",
    "units_list = [10]\n",
    "\n",
    "# QUANTIZATION PARAMS\n",
    "bit_flat = 16\n",
    "\n",
    "bits_qactiv_list  = [bit_flat, bit_flat, bit_flat, bit_flat]\n",
    "bits_qweight_list = [bit_flat, bit_flat, bit_flat, 0       ] # last value is dummy\n",
    "\n",
    "#------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeLayer(Layer):\n",
    "\n",
    "    \"\"\"Subclass of Layer to create an Identity or Fake layer that does not exist for TensorFlow 2.4.0:\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/Identity\"\"\"\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "        super(FakeLayer, self).__init__(name=name)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case without fused BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Keras model that we want to convert in QKeras\n",
    "BatchNorm is not folded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fused_batchnorm == 0:\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        MaxPooling2D(pool_size=pool_size_list[0], name=\"pool\"),\n",
    "\n",
    "        # Example of Conv2D without Batchnorm\n",
    "        Conv2D(filters_list[0], kernel_size_list[0], strides_list[0], pads_list[0], name=\"conv2d_0\"),\n",
    "        ReLU(name=\"relu_0\"),\n",
    "\n",
    "        # Example of Conv2D with Batchnorm\n",
    "        Conv2D(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1], name=\"conv2d_1\"),\n",
    "        BatchNormalization(name=\"bn_1\"), # BatchNorm is not folded\n",
    "        ReLU(name=\"relu_1\"),\n",
    "\n",
    "        Flatten(name=\"flatten\"),\n",
    "\n",
    "        # Example of Dense without Batchnorm\n",
    "        Dense(units_list[0], name=\"dense\"),\n",
    "        Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    model.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "    model.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QKeras model with new activation layer \"quantized_bits_featuremap\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fused_batchnorm == 0:\n",
    "    \n",
    "    qmodel = tf.keras.models.Sequential([\n",
    "\n",
    "        MaxPooling2D(pool_size_list[0], name=\"pool\"),\n",
    "        \n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % \\\n",
    "                    (bits_qactiv_list[0], bits_qactiv_list[0]), name=\"act_0\"),\n",
    "\n",
    "        # Example of Conv2D without Batchnorm\n",
    "        QConv2D(filters_list[0], kernel_size_list[0], strides_list[0], pads_list[0],\n",
    "                kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % \\\n",
    "                                 (bits_qweight_list[0], bits_qweight_list[0]), \n",
    "                bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\", name=\"conv2d_0\"),\n",
    "                #activation=\"relu\"), # This way applies quantized_relu() that we do not want\n",
    "        ReLU(name=\"relu_0\"), # Use this instead\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % \\\n",
    "                    (bits_qactiv_list[1], bits_qactiv_list[1]), name=\"act_1\"),\n",
    "\n",
    "        # Example of Conv2D with fused Batchnorm\n",
    "        #QConv2DBatchnorm(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1],\n",
    "        #                 kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % \\\n",
    "        #                 (bits_qweight_list[1], bits_qweight_list[1]), \n",
    "        #                 bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\", name=\"conv2d_1\"),\n",
    "        #FakeLayer(name=\"bn_1\"),\n",
    "        QConv2D(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1],\n",
    "                kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % \\\n",
    "                (bits_qweight_list[1], bits_qweight_list[1]), \n",
    "                bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\", name=\"conv2d_1\"),\n",
    "        BatchNormalization(name=\"bn_1\"),\n",
    "        ReLU(name=\"relu_1\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % \\\n",
    "                    (bits_qactiv_list[2], bits_qactiv_list[2]), name=\"act_2\"),\n",
    "\n",
    "        Flatten(name=\"flatten\"),\n",
    "\n",
    "        # Example of Dense without Batchnorm (scale_axis=2 for per-layer quantization for Dense)\n",
    "        QDense(units_list[0],\n",
    "               kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto',scale_axis=2)\" % \\\n",
    "                                (bits_qweight_list[2], bits_qweight_list[2]),\n",
    "               bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\", name=\"dense\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % \\\n",
    "                    (bits_qactiv_list[3], bits_qactiv_list[3]), name=\"act_3\"),\n",
    "\n",
    "        Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    qmodel.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "    qmodel.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "    qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case with fused BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Keras model that we want to convert in QKeras\n",
    "BatchNorm is folded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fused_batchnorm == 1:\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        MaxPooling2D(pool_size=pool_size_list[0], name=\"pool\"),\n",
    "\n",
    "        # Example of Conv2D without Batchnorm\n",
    "        Conv2D(filters_list[0], kernel_size_list[0], strides_list[0], pads_list[0], name=\"conv2d_0\"),\n",
    "        ReLU(name=\"relu_0\"),\n",
    "\n",
    "        # Example of Conv2D with Batchnorm\n",
    "        Conv2D(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1], name=\"conv2d_1\"),\n",
    "        # Not needed because folded weights of qmodel will be transfered inside Conv2D (see later cells)\n",
    "        #BatchNormalization(name=\"bn_1\"),\n",
    "        ReLU(name=\"relu_1\"),\n",
    "\n",
    "        Flatten(name=\"flatten\"),\n",
    "\n",
    "        # Example of Dense without Batchnorm\n",
    "        Dense(units_list[0], name=\"dense\"),\n",
    "        Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    model.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "    model.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QKeras model with new activation layer \"quantized_bits_featuremap\"\n",
    "quantized_bits_featuremap(bits,integer,symmetric,keep_negative,alpha,scale_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fused_batchnorm == 1:\n",
    "    \n",
    "    qmodel = tf.keras.models.Sequential([\n",
    "\n",
    "        MaxPooling2D(pool_size_list[0], name=\"pool\"),\n",
    "        \n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % \\\n",
    "                    (bits_qactiv_list[0], bits_qactiv_list[0]), name=\"act_0\"),\n",
    "\n",
    "        # Example of Conv2D without Batchnorm\n",
    "        QConv2D(filters_list[0], kernel_size_list[0], strides_list[0], pads_list[0],\n",
    "                kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % \\\n",
    "                                 (bits_qweight_list[0], bits_qweight_list[0]), \n",
    "                bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\", name=\"conv2d_0\"),\n",
    "                #activation=\"relu\"), # This way applies quantized_relu() that we do not want\n",
    "        ReLU(name=\"relu_0\"), # Use this instead\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % \\\n",
    "                    (bits_qactiv_list[1], bits_qactiv_list[1]), name=\"act_1\"),\n",
    "\n",
    "        # Example of Conv2D with fused Batchnorm\n",
    "        QConv2DBatchnorm(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1],\n",
    "                         kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % \\\n",
    "                         (bits_qweight_list[1], bits_qweight_list[1]), \n",
    "                         bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\", name=\"conv2d_1\"),\n",
    "        FakeLayer(name=\"bn_1\"),\n",
    "        #QConv2D(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1],\n",
    "        #        kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % \\\n",
    "        #        (bits_qweight_list[1], bits_qweight_list[1]), \n",
    "        #        bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\", name=\"conv2d_1\"),\n",
    "        #BatchNormalization(name=\"bn_1\"),\n",
    "        ReLU(name=\"relu_1\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % \\\n",
    "                    (bits_qactiv_list[2], bits_qactiv_list[2]), name=\"act_2\"),\n",
    "\n",
    "        Flatten(name=\"flatten\"),\n",
    "\n",
    "        # Example of Dense without Batchnorm (scale_axis=2 for per-layer quantization for Dense)\n",
    "        QDense(units_list[0],\n",
    "               kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto',scale_axis=2)\" % \\\n",
    "                                (bits_qweight_list[2], bits_qweight_list[2]),\n",
    "               bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\", name=\"dense\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % \\\n",
    "                    (bits_qactiv_list[3], bits_qactiv_list[3]), name=\"act_3\"),\n",
    "\n",
    "        Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    qmodel.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "    qmodel.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "    qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, save, load qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# User settings\n",
    "\n",
    "train_model = 1\n",
    "epochs = 1\n",
    "\n",
    "save_model  = 1\n",
    "\n",
    "if fused_batchnorm == 0:\n",
    "    save_path = \"./qmodel\"\n",
    "else:\n",
    "    save_path = \"./qmodel_fusedbatchnorm\"\n",
    "    \n",
    "#------------------------------------------\n",
    "\n",
    "if train_model == 1:\n",
    "    \n",
    "    qmodel.fit(x_train, y_train, batch_size=512,\n",
    "               epochs=epochs, validation_split=0.25, shuffle=True)    \n",
    "    \n",
    "if save_model == 1:\n",
    "\n",
    "    qmodel.save_weights(save_path, overwrite=True, save_format=\"tf\")\n",
    "    os.remove(\"./checkpoint\")\n",
    "\n",
    "    print(\"Model saved correctly\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    qmodel.load_weights(save_path, by_name=False).expect_partial()\n",
    "    \n",
    "    print(\"Model loaded correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer weights from qmodel to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def my_evaluate(predictions, y):\n",
    "    index_pred = np.argmax(predictions)\n",
    "    index_gold = np.argmax(y)\n",
    "    if index_pred != index_gold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "\n",
    "qlayers = []\n",
    "for qlayer in qmodel.layers:\n",
    "    if qlayer.get_weights():\n",
    "        qlayers.append(qlayer)\n",
    "\n",
    "layers = []\n",
    "for layer in model.layers:\n",
    "    if layer.get_weights():\n",
    "        layers.append(layer)\n",
    "\n",
    "        \n",
    "for qlayer, layer in zip(qlayers, layers):\n",
    "    \n",
    "    # To disable all the printings to stdout of the functions inside this statement: \n",
    "    # https://stackoverflow.com/questions/23610585/ipython-notebook-avoid-printing-within-a-function/23611571\n",
    "    with io.capture_output(stdout=True, stderr=False) as captured:\n",
    "        \n",
    "        print(qlayer.__class__.__name__)\n",
    "\n",
    "        if qlayer.get_weights():\n",
    "            \n",
    "            print(qlayer.name)\n",
    "            print(\"qlayer.get_weights()[0].shape:\", qlayer.get_weights()[0].shape)\n",
    "            print(\"qlayer.get_weights()[1].shape:\", qlayer.get_weights()[1].shape)\n",
    "            print(\"layer.get_weights()[0].shape:\", layer.get_weights()[0].shape)\n",
    "            print(\"layer.get_weights()[1].shape:\", layer.get_weights()[1].shape)\n",
    "            try:\n",
    "                print(\"This layer IS FOLDED\")\n",
    "                extracted_weights = qlayer.get_folded_weights()\n",
    "            except:\n",
    "                print(\"This layer is NOT folded\")\n",
    "                extracted_weights = qlayer.get_weights()[0:2]\n",
    "            \n",
    "            print(\"layer.get_weights()[0][0] MODEL BEFORE\")\n",
    "            print(layer.get_weights()[0][0])\n",
    "            print(\"layer.get_weights()[1] MODEL BEFORE\")\n",
    "            print(layer.get_weights()[1])\n",
    "            layer.set_weights(copy.deepcopy(extracted_weights))\n",
    "            print(\"layer.get_weights()[0][0] MODEL AFTER\")\n",
    "            print(layer.get_weights()[0][0])\n",
    "            print(\"layer.get_weights()[1] MODEL AFTER\")\n",
    "            print(layer.get_weights()[1])\n",
    "            \n",
    "            try:\n",
    "                print(\"qlayer.get_folded_weights()[0][0] QMODEL\")\n",
    "                print(qlayer.get_folded_weights()[0][0])\n",
    "                print(\"qlayer.get_folded_weights()[1] QMODEL\")\n",
    "                print(qlayer.get_folded_weights()[1])\n",
    "            except:\n",
    "                print(\"qlayer.get_weights()[0][0] QMODEL\")\n",
    "                print(qlayer.get_weights()[0][0])\n",
    "                print(\"qlayer.get_weights()[1] QMODEL\")\n",
    "                print(qlayer.get_weights()[1])\n",
    "            \n",
    "            result = layer.get_weights()[0:2]\n",
    "            if not np.array_equal(layer.get_weights()[0], extracted_weights[0]) or \\\n",
    "               not np.array_equal(layer.get_weights()[1], extracted_weights[1]):\n",
    "                raise Exception(\"Transfer weights failed\")\n",
    "\n",
    "        print(\"------------\")\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class=\"anchor\" id=\"ch2\"></a>\n",
    "# 2) Run inference and compare model with qmodel\n",
    "\n",
    "Go to next: [Ch. 3](#ch3).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# User settings\n",
    "\n",
    "samples_to_run = 25\n",
    "\n",
    "offset = 0\n",
    "min_samples = offset\n",
    "max_samples = offset+samples_to_run\n",
    "\n",
    "print_prediction = True\n",
    "\n",
    "#------------------------------------------\n",
    "\n",
    "tot_layers = 0\n",
    "for layer in qmodel.layers:\n",
    "    if layer.get_weights():\n",
    "        tot_layers += 1\n",
    "print(\"tot_layers:\", tot_layers)\n",
    "\n",
    "iterations = 0\n",
    "test_acc_accumulator_model = 0\n",
    "test_acc_accumulator_qmodel = 0\n",
    "\n",
    "for x, y in zip(x_test[min_samples:max_samples], y_test[min_samples:max_samples]):\n",
    "        \n",
    "    x_reshaped = x.reshape(1, x.shape[0], x.shape[1], x.shape[2])\n",
    "    \n",
    "    # Predict the samples with model, i.e. the Keras model\n",
    "    pred_model = np.asarray(model.predict(x_reshaped, batch_size=1, verbose=0), dtype=np.float64)\n",
    "\n",
    "    # Predict the samples with qmodel, i.e. the modified QKeras model\n",
    "    pred_qmodel = np.asarray(qmodel.predict(x_reshaped, batch_size=1, verbose=0), dtype=np.float64)\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_acc_model  = my_evaluate(pred_model,  y)\n",
    "    test_acc_qmodel = my_evaluate(pred_qmodel, y)\n",
    "\n",
    "    # Check predictions\n",
    "    if print_prediction == True:\n",
    "        \n",
    "        print(\"\\npred_model:\\n\", pred_model)\n",
    "        print(\"pred_qmodel:\\n\", pred_qmodel)\n",
    "        \n",
    "        print(\"test_acc_model:   \", test_acc_model)\n",
    "        print(\"test_acc_qmodel:  \", test_acc_qmodel)\n",
    "        \n",
    "    test_acc_accumulator_model  += test_acc_model\n",
    "    test_acc_accumulator_qmodel += test_acc_qmodel\n",
    "\n",
    "    iterations = iterations + 1\n",
    "    \n",
    "    if print_prediction == True:\n",
    "        print(\"iteration %d/%d\" % (iterations, (max_samples - min_samples)))\n",
    "        print(\"-------------------------\")\n",
    "    \n",
    "print(\"-------------------------\")\n",
    "\n",
    "print(\"TOT iterations:        \", iterations)\n",
    "print(\"TOT test_acc_model:    \", test_acc_accumulator_model/iterations)\n",
    "print(\"TOT test_acc_qmodel:   \", test_acc_accumulator_qmodel/iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "<a class=\"anchor\" id=\"ch3\"></a>\n",
    "# 3) Extract QKeras quantization factors\n",
    "Go to next: [Ch. 4](#ch4).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top).\n",
    "\n",
    "This is the complete quantization formula for a matrix multiplication operation (valid also for an FC layer) taken from [[3]#Quantized-Matrix-Multiplication-Mathematics](#https://leimao.github.io/article/Neural-Networks-Quantization/#Quantized-Matrix-Multiplication-Mathematics) (an equivalent version is Eq.7 in [2]):\n",
    "$$\\begin{align} Y_{q,i,j} &= z_Y + \\frac{s_b}{s_Y} (b_{q, j} - z_b) + \\frac{s_X s_W}{s_Y} \\Bigg[ \\bigg( \\sum_{k=1}^{p} X_{q,i,k} W_{q, k,j} \\bigg) - \\bigg( z_W \\sum_{k=1}^{p} X_{q,i,k} \\bigg) - \\bigg( z_X \\sum_{k=1}^{p} W_{q, k,j} \\bigg) + p z_X z_W\\Bigg] \\end{align}$$\n",
    "\n",
    "There are some contributions that could be deleted if the <b>zero points</b> of weights ```z_w``` and biases ```z_b``` <b>are forced to be zero, i.e. if both the quantized range and the fake-quantized/floating-point range of weights and biases, respectively, are symmetric</b>. When this happens, affine quantization mapping is called scale quantization mapping [3]. It is relatively easy to set the quantized range to be symmetric (for example, in QKeras we just need to pass ```quantized_bits()``` with ```symmetric=1``` and ```keep_negative=1``` to both the arguments ```kernel_quantizer``` and ```bias_quantizer``` of each QKeras layer), but this is not the case for the fake-quantized/floating-point range. There are two ways to make the latter symmetric:\n",
    "\n",
    "1) during training, by constraining weight and bias tensors to a given symmetric range of values;\n",
    "\n",
    "2) during training, by using a different way to calculate the scaling factor ```s```. Instead of calculating it in the standard and more general way: \n",
    "$$\\begin{align} s &= \\frac{\\beta - \\alpha}{\\beta_q - \\alpha_q}\\end{align},$$\n",
    "it can be calculated as: \n",
    "$$\\begin{align} s &= \\frac{2 * max (abs (tensor) )}{\\beta_q - \\alpha_q}\\end{align},$$\n",
    "where ```tensor``` is the floating-point weight/bias tensor to be quantized, ```[alpha; beta]``` is the floating-point range (where in turns ```alpha``` and ```beta``` are the minimum and maximum values of the entire tensor, so there is only one scalar ```s``` for the entire tensor), ```[alphaq; betaq]``` is the quantized range (which depends on the number of bits we want to represent the quantized data). Regarding the operations, ```abs()``` calculates the absolute value of all the elements in ```tensor``` and ```max``` extracts the maximum value from each channel (so in the second formula ```s``` is an array of scaling factors). Apart from the difference related to the per-layer vs per-channel quantization, the second formula is more general because removes the constraint of searching for the minimum in the tensor and directly assumes that the floating-point range (numerator) is symmetric, even if it is not actually true, but in this way it avoids to constrain ```alpha``` and ```beta``` to be exactly equal and opposite.\n",
    "\n",
    "[TensorFlow Lite states](https://www.tensorflow.org/lite/performance/quantization_spec#symmetric_vs_asymmetric) that they are forcing the zero points to zero, but they do not show how (maybe it is necessary to look at the source code: future work). [This guy](https://stackoverflow.com/questions/69746834/tf-lite-model-force-symmetric-filter-weights-in-fully-connected-layers) tried to implement the first approach using [tf.keras.constraints](https://www.tensorflow.org/api_docs/python/tf/keras/constraints) without success; instead QKeras follows the second approach, as you can see in source code of ```quantized_bits()``` in [quantizers.py#L586](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L586).\n",
    "\n",
    "In the light of the aforementioned, in the next cells of this notebook we will extract and save to csv files only the following quantization parameters that will be needed for the inference phase:\n",
    "\n",
    "- ```wq``` and ```scale_w``` are the quantized weights and their scaling factors (known after training (zero-point weights = 0));\n",
    "\n",
    "- ```bq``` and ```scale_b``` are the quantized biases and their scaling factors (known after training (zero-point biases = 0));\n",
    "\n",
    "- ```subq1``` is the third term in the squared brackets in the quantization formula above, i.e. the summation over the quantized weights multiplied by the zero point of the input features (known after training):\n",
    "$$\\bigg( z_X \\sum_{k=1}^{p} W_{q, k,j} \\bigg)$$\n",
    "\n",
    "- ```scale_f```, ```zeropoint_f``` are the scaling factors and the zero points of the activation layers (need calibration to get maximum absolute values of alpha and beta for each layer).\n",
    "\n",
    "To run the notebook without issues, you should edit the file [quantizers.py](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py) to expose the following internal variables to the external world as attributes (to ease this step, just follow the instructions in the readme file of this repo):\n",
    "\n",
    "- ```m_i = K.cast_to_floatx(K.pow(2, self.integer))```;\n",
    "\n",
    "- ```scale1 = (K.max(abs(x), axis=axis, keepdims=True) * 2) / levels```.\n",
    "\n",
    "In particular, we need ```m_i``` and ```scale1``` to compute a scaling factor that matches the definition of scaling factor of TensorFlow Lite [2][3]. In fact, one might imagine that the scaling factor provided by the ```scale``` attribute of ```quantized_bits()``` is the same as the TensorFlow one: unfortunately it is not, as you can see from [quantizers.py#L608](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L608). The correct scaling factor is ```scale = scale1 * m_i``` and has to be computed manually.\n",
    "\n",
    "Finally, the following calculations show the <b>quantization of the weights</b>, which is a <b>per-channel</b>  approach, i.e. weights have a number of scaling factors and zero points equal to the number of output channels, while <b>activations are quantized in a per-layer fashion</b> (one scaling factor and one zero point for each feature map tensor). The difference is the use of ```scale_axis=0```in ```quantized_bits()``` for ```QActivation()```. The reason why per-channel quantization of activations is not implemented in QKeras, as well as in TensorFlow Lite, is because \"<i>per-channel quantization of activations is much harder to implement because we cannot factor the scale factor out of the summation and would, therefore, require rescaling the accumulator for each input channel</i>\" [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_calibration_data(model, x_test, calibration_samples, csv_file_path_w, csv_file_path_a):\n",
    "\n",
    "    \"\"\"\n",
    "    Return two dictionaries: one for weights and biases, and one for activations\n",
    "    1) weight and bias: scale and zero-point are known after training (zero-point = 0)\n",
    "    2) activations: scale and bias vary with the input -->\n",
    "                   --> need calibration to get maximum absolute values of alpha and beta\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Define the weights and biases dictionary\n",
    "    base_param_dict = {\n",
    "               \"w_scale\": 0,\n",
    "               \"b_scale\": 0,\n",
    "               \"subq1\": [],\n",
    "               \"wq\": [],\n",
    "               \"bq\": []\n",
    "    }\n",
    "    \n",
    "    w_layers = []\n",
    "    for layer in model.layers:\n",
    "        if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\", \n",
    "                                        \"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\", \n",
    "                                        \"QDense\", \"QDenseBatchnorm\"]:\n",
    "            w_layers.append(layer.name)\n",
    "\n",
    "    # Deepcopy base dict otherwise it is always the same object\n",
    "    w_dict = {k: dc(base_param_dict) for k in w_layers}\n",
    "\n",
    "    \n",
    "    # Define the activations dictionary\n",
    "    base_act_dict = {\n",
    "        \"alpha_of_max_abs\": [],\n",
    "        \"beta_of_max_abs\": [],\n",
    "        \"in_scale\": [],\n",
    "        \"in_zeropoint\":[]\n",
    "    }\n",
    "    \n",
    "    a_layers = []\n",
    "    for layer in model.layers:\n",
    "        if layer.__class__.__name__ in [\"QActivation\"]:\n",
    "            a_layers.append(layer.name)\n",
    "    \n",
    "    a_dict = {k: dc(base_act_dict) for k in a_layers}   \n",
    "                       \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract maxium absolute values of alpha and beta of activations with calibration\n",
    "    for layer in model.layers:\n",
    "    \n",
    "        alpha_of_list = []\n",
    "        beta_of_list = []\n",
    "        \n",
    "        ##### FEATURES #####\n",
    "        if layer.__class__.__name__ in [\"QActivation\"]:\n",
    "                \n",
    "            for iter, x in enumerate(x_test[0:calibration_samples]):\n",
    "\n",
    "                data = x.reshape(1, x.shape[0], x.shape[1], x.shape[2])\n",
    "\n",
    "                pred_model = np.asarray(model.predict(data, batch_size=1, verbose=0), dtype=np.float64)\n",
    "\n",
    "                quantizer_of = layer.quantizer # it is quantized_bits_featuremap\n",
    "                \n",
    "                alpha_of = quantizer_of.alpha_f.numpy().flatten()\n",
    "                beta_of = quantizer_of.beta_f.numpy().flatten()\n",
    "                alpha_of_list.append(alpha_of)\n",
    "                beta_of_list.append(beta_of)\n",
    "                        \n",
    "            alpha_of_max_abs = np.max(np.abs(alpha_of_list))\n",
    "            beta_of_max_abs = np.max(np.abs(beta_of_list))\n",
    "            \n",
    "            print(f\"layer.name: {layer.name}, \\t alpha_of_max_abs:\", alpha_of_max_abs, \\\n",
    "                  \"\\tbeta_of_max_abs:\", beta_of_max_abs)\n",
    "            \n",
    "            a_dict[layer.name][\"alpha_of_max_abs\"].append(alpha_of_max_abs)\n",
    "            a_dict[layer.name][\"beta_of_max_abs\"].append(beta_of_max_abs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Extract weights, biases and activations\n",
    "    zeropoint_of_list = []\n",
    "    ready1 = 0\n",
    "    \n",
    "    x = x_test[0]\n",
    "    data = x.reshape(1, x.shape[0], x.shape[1], x.shape[2])\n",
    "\n",
    "    pred_model = np.asarray(model.predict(data, batch_size=1, verbose=0), dtype=np.float64)\n",
    "\n",
    "    for iter, layer in enumerate(model.layers):\n",
    "        extractor = tf.keras.Model( inputs=model.inputs,\n",
    "                                   outputs=model.get_layer(layer.name).output)\n",
    "        of = extractor(data).numpy()\n",
    "        \n",
    "        ##### WEIGHTS AND BIASES #####\n",
    "        if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\", \n",
    "                                        \"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\", \n",
    "                                        \"QDense\", \"QDenseBatchnorm\"]:\n",
    "\n",
    "            ##### WEIGHTS #####\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QDepthwiseConv2D\", \"QDense\"]:\n",
    "                parameters = layer.weights\n",
    "            else:\n",
    "                parameters = layer.get_folded_weights() # folded weights not quantized\n",
    "\n",
    "            w = parameters[0].numpy()\n",
    "            quantizer_w = layer.get_quantizers()[0] # it is quantized_bits\n",
    "            alphaq_w = quantizer_w.alphaq\n",
    "            betaq_w = quantizer_w.betaq\n",
    "            scale1_w = quantizer_w.scale1.numpy().flatten()\n",
    "            m_i = quantizer_w.m_i.numpy().flatten()\n",
    "            scale_w = scale1_w * m_i # WEIGHT SCALE\n",
    "\n",
    "            if scale_w.any() != 0:\n",
    "                tmp = np.divide(w, scale_w, dtype=np.float64)\n",
    "                tmp[np.isnan(tmp)] = 0\n",
    "                tmp[np.isinf(tmp)] = 0\n",
    "                wq = np.clip(np.trunc(tmp + np.sign(tmp)*0.5), alphaq_w, betaq_w) # QUANTIZED WEIGHTS\n",
    "            else:\n",
    "                wq = np.zeros(w.size)\n",
    "                raise Exception(\"scale_w has some values equal to 0.\")\n",
    "                \n",
    "\n",
    "            # \"subq1\" (part 2)\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\"]:\n",
    "                sum_of_weights = wq.sum(axis=(0, 1, 2))\n",
    "            elif layer.__class__.__name__ in [\"QDense\", \"QDenseBatchnorm\"]:\n",
    "                sum_of_weights = wq.sum(axis=0)\n",
    "            if layer.__class__.__name__ in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]:\n",
    "                sum_of_weights = wq.sum(axis=(0, 1))\n",
    "\n",
    "            if ready1 == 1:\n",
    "                subq1 = (zeropoint_of_list[-1] * sum_of_weights).flatten()\n",
    "                ready1 = 0\n",
    "                w_dict[layer.name][\"subq1\"] = subq1\n",
    "\n",
    "\n",
    "            ##### BIASES ######\n",
    "            quantizer_b = layer.get_quantizers()[1] # it is quantized_bits\n",
    "            b = parameters[1].numpy()\n",
    "            alphaq_b = quantizer_b.alphaq\n",
    "            betaq_b = quantizer_b.betaq\n",
    "            scale1_b = quantizer_b.scale1.numpy().flatten()\n",
    "            m_i = quantizer_b.m_i.numpy().flatten()\n",
    "            scale_b = scale1_b * m_i # BIAS SCALE\n",
    "\n",
    "\n",
    "            if scale_b != 0:\n",
    "                tmp = np.divide(b, scale_b, dtype=np.float64)\n",
    "                tmp[np.isnan(tmp)] = 0\n",
    "                tmp[np.isinf(tmp)] = 0\n",
    "                bq = np.clip(np.trunc(tmp + np.sign(tmp)*0.5), alphaq_b, betaq_b) # QUANTIZED BIASES\n",
    "            else:\n",
    "                bq = np.zeros(b.size)\n",
    "\n",
    "            w_dict[layer.name]['w_scale'] = scale_w\n",
    "            w_dict[layer.name][\"b_scale\"] = scale_b\n",
    "            w_dict[layer.name][\"wq\"] = wq\n",
    "            w_dict[layer.name][\"bq\"] = bq\n",
    "\n",
    "            \n",
    "        ##### FEATURES #####\n",
    "        elif layer.__class__.__name__ in [\"QActivation\"]:\n",
    "\n",
    "            quantizer_of = layer.quantizer # it is quantized_bits_featuremap\n",
    "            alphaq_of = quantizer_of.alphaq\n",
    "            betaq_of = quantizer_of.betaq\n",
    "\n",
    "            alpha_of_max_abs = a_dict[layer.name][\"alpha_of_max_abs\"][0]\n",
    "            beta_of_max_abs = a_dict[layer.name][\"beta_of_max_abs\"][0]\n",
    "                        \n",
    "            scale_of = np.asarray([(beta_of_max_abs - alpha_of_max_abs) / \\\n",
    "                                   (betaq_of - alphaq_of)], dtype=np.float64)\n",
    "            scale_of[np.isnan(scale_of)] = 0\n",
    "            scale_of[np.isinf(scale_of)] = 0\n",
    "            scale_of = scale_of[0] # ACTIVATION SCALE\n",
    "\n",
    "            z_of = np.asarray([np.around(((beta_of_max_abs*alphaq_of - alpha_of_max_abs*betaq_of)/ \\\n",
    "                                          (beta_of_max_abs - alpha_of_max_abs)), 0)], dtype=np.float64)\n",
    "            z_of[np.isnan(z_of)] = 0\n",
    "            z_of[np.isinf(z_of)] = 0\n",
    "            z_of = z_of[0] # ACTIVATION ZERO-POINT\n",
    "            zeropoint_of_list.append(z_of)\n",
    "            \n",
    "            ready1 = 1 # \"subq1\" (part 1)\n",
    "\n",
    "            a_dict[layer.name][\"in_scale\"].append(scale_of)\n",
    "            a_dict[layer.name][\"in_zeropoint\"].append(z_of)\n",
    "\n",
    "        print(f\"{iter+1}/{len(model.layers)} \\t{layer.name}\")\n",
    "        \n",
    "        \n",
    " \n",
    "    # Index will be determined by the first layer of nested dictionaries (layers)\n",
    "    df_w = pd.DataFrame.from_dict(w_dict, orient=\"index\")\n",
    "    df_a = pd.DataFrame.from_dict(a_dict, orient=\"index\")\n",
    "    \n",
    "    df_w.to_csv(csv_file_path_w)\n",
    "    df_a.to_csv(csv_file_path_a)\n",
    "    \n",
    "    return (df_w, df_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# User settings\n",
    "\n",
    "calibration_samples = 10\n",
    "\n",
    "#------------------------------------------\n",
    "\n",
    "output_folder = \"./calibration_results\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(output_folder)\n",
    "except:\n",
    "    print(\"folder exists\")\n",
    "    \n",
    "\n",
    "(df_w, df_a) = extract_calibration_data(qmodel, x_test, calibration_samples,\n",
    "                                        output_folder+\"/extracted_weights.csv\",\n",
    "                                        output_folder+\"/extracted_activations.csv\")\n",
    "\n",
    "print(\"Extraction complete\")\n",
    "\n",
    "display(df_w)\n",
    "display(df_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class=\"anchor\" id=\"ch4\"></a>\n",
    "# 4) Quantized network design for AutoQKeras\n",
    "\n",
    "Go to [Top](#top).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "To perform an hyperparameter search on a Keras model with AutoQKeras, we need to pass the Keras model to the first argument of the AutoQKeras class to create an AutoQKeras object. The input Keras model is automatically converted to a QKeras model during the building process by the ```quantize_model()``` method of AutoQKHyperModel class (see [autoqkeras_internal.py#L570](https://github.com/google/qkeras/blob/1ab354276a041b45cd72c300e89a7c51ec99fa35/qkeras/autoqkeras/autoqkeras_internal.py#L570)) (```quantize_model()``` does not accept an input model with QKeras layers).\n",
    "\n",
    "The idea is to exploit this automatic convertion to realize a QKeras model according to the methodology previously described in this notebook ([Ch. 1](#ch1)) with as minimum changes as possible to the original Keras model definition, as explained here:\n",
    "\n",
    "1) Every 2DConv, DWConv and FC layer has to be anticipated by an ```Activation``` layer with whatever activation function (we don't care about the type of activation because it will be replaced by AutoQKeras during the search with the ```activation``` values defined in the search space ```quantization_config```). In this example we are going to use \"sigmoid\", but any other type would be fine;\n",
    "\n",
    "2) The last 2DConv, DWConv or FC layer of the network has also to be followed by an ```Activation``` layer, as written in the previous point;\n",
    "\n",
    "3) Every ```BatchNormalization``` layer that follows a 2DConv or DWConv layer has to be fused with the convolution. We can use the flag ```enable_bn_folding=True``` when instantiating the AutoQKeras object to automatically do the batch normalization fusion;\n",
    "\n",
    "4) Every activation layer associated to a convolutional or fully-connected layer, either declared as argument of the ```Activation``` layer (such as ```Activation(activation=\"relu\")```), or declared as argument of the ```Conv2D```, ```DepthwiseConv2D``` or ```Dense``` layer (such as ```Conv2D(activation=\"relu\")```), has to be written in the \"direct\" form, i.e. using a layer that has the same name of the activation (such as ```ReLU()```);\n",
    "\n",
    "5) The activations not associated to a convolutional or fully-connected layer, usually those placed as last layer of CNNs (such as softmax), has not to be changed;\n",
    "\n",
    "6) Any other layer, such as pooling or add layers, has not to be changed;\n",
    "\n",
    "7) After average pooling, since the average of integers is not necessarily an integer, a quantization step is required (it would be better to use the same quantizer used for its input because its output range does not significantly change). Moreover, the dynamic of the activations changes between input and output of pooling. So an ```Activation``` layer is required after an average pooling;\n",
    "\n",
    "8) For residual connections (\"add\" layers), do as the next cell shows.\n",
    "\n",
    "To run the next cells without issues, you should edit the file [autoqkeras/autoqkeras_internal.py](https://github.com/google/qkeras/blob/master/qkeras/autoqkeras/autoqkeras_internal.py) to expose the flag ```enable_bn_folding``` to the AutoQKeras interface (externally) and to connect it to the ```AutoQKHyperModel``` class and its ```model_quantize``` method (internally). (To ease this step, just follow the instructions in the readme file of this repo)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example of residual connection (\"add\" layer)\n",
    "\n",
    "...\n",
    "\n",
    "# First stack\n",
    "\n",
    "num_filters = 16\n",
    "x = Activation('sigmoid')(inputs)\n",
    "x = Conv2D(num_filters,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=l2(1e-4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "x = Activation('sigmoid')(x)\n",
    "\n",
    "y = Conv2D(num_filters,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=l2(1e-4))(x)\n",
    "y = BatchNormalization()(y)\n",
    "y = ReLU()(y)\n",
    "y = Activation('sigmoid')(y)\n",
    "\n",
    "x = tf.keras.layers.add([x, y])\n",
    "x = ReLU()(x)\n",
    "x = Activation('sigmoid')(x)\n",
    "\n",
    "# Second stack\n",
    "\n",
    "num_filters = 32\n",
    "y = Conv2D(num_filters,\n",
    "            kernel_size=3,\n",
    "            strides=2,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=l2(1e-4))(x)\n",
    "...\n",
    "\n",
    "x = Conv2D(num_filters,\n",
    "        kernel_size=1,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=l2(1e-4))(x)\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pool (MaxPooling2D)          (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "act_0 (Activation)           (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_0 (Conv2D)            (None, 5, 5, 2)           20        \n",
      "_________________________________________________________________\n",
      "relu_0 (ReLU)                (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "act_1 (Activation)           (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 3, 3)           57        \n",
      "_________________________________________________________________\n",
      "bn_1 (BatchNormalization)    (None, 3, 3, 3)           12        \n",
      "_________________________________________________________________\n",
      "relu_1 (ReLU)                (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "act_2 (Activation)           (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                280       \n",
      "_________________________________________________________________\n",
      "act_3 (Activation)           (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 369\n",
      "Trainable params: 363\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 23:36:06.503268: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "if fused_batchnorm == 1:\n",
    "    \n",
    "    model2 = tf.keras.models.Sequential([\n",
    "\n",
    "        MaxPooling2D(pool_size=pool_size_list[0], name=\"pool\"),\n",
    "        Activation(\"sigmoid\", name=\"act_0\"), # fake activation layer\n",
    "\n",
    "        # Example of Conv2D without Batchnorm\n",
    "        Conv2D(filters_list[0], kernel_size_list[0], strides_list[0], pads_list[0], name=\"conv2d_0\"),\n",
    "        ReLU(name=\"relu_0\"),\n",
    "        Activation(\"sigmoid\", name=\"act_1\"), # fake activation layer\n",
    "\n",
    "        # Example of Conv2D with Batchnorm\n",
    "        Conv2D(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1], name=\"conv2d_1\"),\n",
    "        # This time is needed because the folding will be carried out by AutoQKeras\n",
    "        BatchNormalization(name=\"bn_1\"),\n",
    "        ReLU(name=\"relu_1\"),\n",
    "        Activation(\"sigmoid\", name=\"act_2\"), # fake activation layer\n",
    "\n",
    "        Flatten(name=\"flatten\"),\n",
    "\n",
    "        # Example of Dense without Batchnorm\n",
    "        Dense(units_list[0], name=\"dense\"),\n",
    "        Activation(\"sigmoid\", name=\"act_3\"), # fake activation layer\n",
    "        \n",
    "        Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    model2.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "    model2.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "    model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit configuration:{\"Dense\": [16, 31, 16], \"Conv2D\": [16, 31, 16], \"DepthwiseConv2D\": [16, 31, 16], \"Activation\": [16], \"BatchNormalization\": []}\n",
      "INFO:tensorflow:Reloading Oracle from existing project ././autoqkeras/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ././autoqkeras/oracle.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.0010000000474974513\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pool_input (InputLayer)      [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "pool (MaxPooling2D)          (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "act_0 (QActivation)          (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_0 (QConv2D)           (None, 5, 5, 2)           20        \n",
      "_________________________________________________________________\n",
      "relu_0 (ReLU)                (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "act_1 (QActivation)          (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (QConv2DBatchnorm)  (None, 3, 3, 3)           70        \n",
      "_________________________________________________________________\n",
      "relu_1 (ReLU)                (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "act_2 (QActivation)          (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense (QDense)               (None, 10)                280       \n",
      "_________________________________________________________________\n",
      "act_3 (QActivation)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 370\n",
      "Trainable params: 363\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n",
      "stats: delta_p=0.05 delta_n=0.05 rate=2.0 trial_size=8504 reference_size=8193\n",
      "       delta=-0.27%\n",
      "       a_bits=6896/2256 (205.67%) p_bits=1608/5937 (-72.92%)\n",
      "       total=8504/8193 (3.80%)\n",
      "act_0                quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "conv2d_0             f=2 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_1                quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "conv2d_1             f=3 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_2                quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "dense                u=10 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_3                quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "\n",
      "INFO:tensorflow:Reloading Tuner from ././autoqkeras/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from ././autoqkeras/tuner0.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 10\n",
      "conv2d_0_kernel_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto')\", \"quantized_bits(8,8,1,1,alpha='auto')\", \"quantized_bits(16,16,1,1,alpha='auto')\"], 'ordered': False}\n",
      "conv2d_1_kernel_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto')\", \"quantized_bits(8,8,1,1,alpha='auto')\", \"quantized_bits(16,16,1,1,alpha='auto')\"], 'ordered': False}\n",
      "dense_kernel_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto')\", \"quantized_bits(8,8,1,1,alpha='auto')\", \"quantized_bits(16,16,1,1,alpha='auto')\"], 'ordered': False}\n",
      "act_0_activation_quantizer (Choice)\n",
      "{'default': \"quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\", 'conditions': [], 'values': [\"quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits_featuremap(8,8,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\"], 'ordered': False}\n",
      "conv2d_0_bias_quantizer (Choice)\n",
      "{'default': \"quantized_bits(16,16,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(16,16,1,1,alpha='auto')\", \"quantized_bits(31,31,1,1,alpha='auto')\"], 'ordered': False}\n",
      "act_1_activation_quantizer (Choice)\n",
      "{'default': \"quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\", 'conditions': [], 'values': [\"quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits_featuremap(8,8,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\"], 'ordered': False}\n",
      "conv2d_1_bias_quantizer (Choice)\n",
      "{'default': \"quantized_bits(16,16,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(16,16,1,1,alpha='auto')\", \"quantized_bits(31,31,1,1,alpha='auto')\"], 'ordered': False}\n",
      "act_2_activation_quantizer (Choice)\n",
      "{'default': \"quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\", 'conditions': [], 'values': [\"quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits_featuremap(8,8,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\"], 'ordered': False}\n",
      "dense_bias_quantizer (Choice)\n",
      "{'default': \"quantized_bits(16,16,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(16,16,1,1,alpha='auto')\", \"quantized_bits(31,31,1,1,alpha='auto')\"], 'ordered': False}\n",
      "act_3_activation_quantizer (Choice)\n",
      "{'default': \"quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\", 'conditions': [], 'values': [\"quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits_featuremap(8,8,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\"], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "# User settings\n",
    "\n",
    "max_trials = 5\n",
    "epochs_per_trial = 1\n",
    "\n",
    "#------------------------------------------\n",
    "\n",
    "# scale_axis=0 does a per-layer quantization, i.e. one scaling factor for the entire layer\n",
    "# otherwise qkeras automatically does a per-channel quantization, \n",
    "# i.e. one scaling facor for each channel in layer\n",
    "# we want a per-channel quantization for weigths/biases and a per-layer quantization for activations\n",
    "\n",
    "quantization_config = {\n",
    "    \"kernel\": {\n",
    "        \"quantized_bits(4,4,1,1,alpha='auto')\": 4,\n",
    "        \"quantized_bits(8,8,1,1,alpha='auto')\": 8,\n",
    "        \"quantized_bits(16,16,1,1,alpha='auto')\": 16,\n",
    "    },\n",
    "    \"bias\": {\n",
    "        \"quantized_bits(16,16,1,1,alpha='auto')\": 16,\n",
    "        \"quantized_bits(31,31,1,1,alpha='auto')\": 31,\n",
    "    },\n",
    "    \"activation\": {\n",
    "        \"quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\": 4,\n",
    "        \"quantized_bits_featuremap(8,8,1,1,alpha='auto',scale_axis=0)\": 8,\n",
    "        \"quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\": 16\n",
    "    }\n",
    "}\n",
    "\n",
    "# Maximum values for w, b, a\n",
    "limit = {\n",
    "    \"Dense\": [16, 31, 16],\n",
    "    \"Conv2D\": [16, 31, 16],\n",
    "    \"DepthwiseConv2D\": [16, 31, 16],\n",
    "    \"Activation\": [16],\n",
    "    \"BatchNormalization\": []\n",
    "}\n",
    "\n",
    "goal = {\n",
    "    \"type\": \"bits\",\n",
    "    \"params\": {\n",
    "        \"delta_p\": 5.0,\n",
    "        \"delta_n\": 5.0,\n",
    "        \"rate\": 2.0,\n",
    "        \"stress\": 1.0,\n",
    "        \"input_bits\": 8,\n",
    "        \"output_bits\": 8,\n",
    "        \"ref_bits\": 16,\t \n",
    "        \"config\": {\n",
    "            \"default\": [\"parameters\", \"activations\"]\n",
    "        } \n",
    "    }\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "  \"output_dir\": \"./autoqkeras\",\n",
    "  \"goal\": goal,\n",
    "  \"quantization_config\": quantization_config,\n",
    "  \"learning_rate_optimizer\": False,\n",
    "  \"transfer_weights\": True,\n",
    "  \"mode\": \"bayesian\",\n",
    "  \"seed\": 42,\n",
    "  \"limit\": limit,\n",
    "  \"tune_filters\": \"none\",\n",
    "  \"tune_filters_exceptions\": \"none\",\n",
    "  \"distribution_strategy\": tf.distribute.get_strategy(),\n",
    "  \"layer_indexes\": range(1, len(model2.layers) - 1),\n",
    "  \"max_trials\": max_trials\n",
    "}\n",
    "\n",
    "autoqk_model = AutoQKeras(model2, metrics=[\"acc\"], custom_objects={}, \n",
    "                          **run_config, overwrite=False, enable_bn_folding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 06s]\n",
      "val_score: 0.5053437117068068\n",
      "\n",
      "Best val_score So Far: 0.5053437117068068\n",
      "Total elapsed time: 00h 00m 39s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "autoqk.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=512, epochs=epochs_per_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate: 0.0010000000474974513\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pool_input (InputLayer)      [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "pool (MaxPooling2D)          (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "act_0 (QActivation)          (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_0 (QConv2D)           (None, 5, 5, 2)           20        \n",
      "_________________________________________________________________\n",
      "relu_0 (ReLU)                (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "act_1 (QActivation)          (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (QConv2DBatchnorm)  (None, 3, 3, 3)           70        \n",
      "_________________________________________________________________\n",
      "relu_1 (ReLU)                (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "act_2 (QActivation)          (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense (QDense)               (None, 10)                280       \n",
      "_________________________________________________________________\n",
      "act_3 (QActivation)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 370\n",
      "Trainable params: 363\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n",
      "stats: delta_p=0.05 delta_n=0.05 rate=2.0 trial_size=10184 reference_size=8193\n",
      "       delta=-1.57%\n",
      "       a_bits=7928/2256 (251.42%) p_bits=2256/5937 (-62.00%)\n",
      "       total=10184/8193 (24.30%)\n",
      "act_0                quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\n",
      "conv2d_0             f=2 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_1                quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "conv2d_1             f=3 quantized_bits(16,16,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_2                quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\n",
      "dense                u=10 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_3                quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\n",
      "\n",
      "act_0                quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\n",
      "conv2d_0             f=2 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_1                quantized_bits_featuremap(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "conv2d_1             f=3 quantized_bits(16,16,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_2                quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\n",
      "dense                u=10 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_3                quantized_bits_featuremap(16,16,1,1,alpha='auto',scale_axis=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qmodel2 = autoqk_model.get_best_model()  \n",
    "print_qmodel_summary(qmodel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Go to next: [Ch. 5](#ch5).-->\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
