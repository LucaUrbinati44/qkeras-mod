{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"top\"></a>\n",
    "# QKeras-Mod Explained\n",
    "Author: Luca Urbinati, Date: 18/01/2024, v.1.0, Email: luca.urbinati@polito.it\n",
    "***\n",
    "\n",
    "### Content of this notebook:\n",
    "[Chapter 1](#ch1): how to design a quantized model (with and without fused batch normalization) using a modified version of QKeras [1] that quantizes weights <b>and activations</b> to integers starting from a Keras model;\n",
    "\n",
    "[Chapter 2](#ch2): <b>compare inference results</b> between the Keras model and the quantized one;\n",
    "\n",
    "[Chapter 3](#ch3): how to <b>extract quantization factors</b> (scaling factors and zero points) from each layer of the QKeras model to match the uniform quantization theory of Tensorflow Lite [2][3];\n",
    "\n",
    "[Chapter 4](#ch4): how to use <b>AutoQKeras</b> to search for the best mixed-precision quantized model.\n",
    "\n",
    "### Requirements before to start\n",
    "- Read [this QKeras tutorial](https://github.com/google/qkeras/blob/master/notebook/QKerasTutorial.ipynb) to become confident with QKeras.\n",
    "- Install the conda environment [qkeras-env.yml](https://github.com/LucaUrbinati44/qkeras-mod/blob/main/qkeras-env.yml) provided in this repo and activate it (_conda activate qkeras-env_).\n",
    "- Apply the patch to QKeras' installation to have access to the modified version of QKeras (see the [README](https://github.com/LucaUrbinati44/qkeras-mod/blob/main/README.md)). \n",
    "\n",
    "### Main features of QKeras:\n",
    "- quantization-aware training\n",
    "- per-channel quantization for weights (not for activations [4])\n",
    "- by default weights quantization; if properly used, activations quantization too\n",
    "- it uses affine quantization mapping formula, uniform quantization and 2*max(abs(tensor)) as floating-point range instead of the most common max-min\n",
    "\n",
    "### Publications using this code\n",
    "- Luca Urbinati and Mario R. Casu, \"High-Level Design of Precision-Scalable DNN Accelerators Based on Sum-Together Multiplier\", in the review process.\n",
    "\n",
    "### References\n",
    "[1] QKeras: https://github.com/google/qkeras\n",
    "\n",
    "[2] B. Jacob et al., \"Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference,\" arXiv:1712.05877 [cs, stat], Dec. 2017. Available: http://arxiv.org/abs/1712.05877\n",
    "\n",
    "[3] Mao, Lei. \"Quantization for Neural Networks\". Lei Mao’s Log Book, May 17, 2020, https://leimao.github.io/article/Neural-Networks-Quantization/\n",
    "\n",
    "[4] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van Baalen, and T. Blankevoort, “A White Paper on Neural Network Quantization.” arXiv, Jun. 15, 2021. Available: http://arxiv.org/abs/2106.08295\n",
    "\n",
    "[5] H. Wu, P. Judd, X. Zhang, M. Isaev, and P. Micikevicius, “Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation,” arXiv:2004.09602 [cs, stat], Apr. 2020, Accessed: Dec. 22, 2021. [Online]. Available: http://arxiv.org/abs/2004.09602."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class=\"anchor\" id=\"ch0\"></a>\n",
    "# 0) Import libraries, define functions and create folders\n",
    "\n",
    "Go to next: [Ch. 1](#ch1).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 17:39:57.306367: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from IPython.utils import io\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from qkeras import *\n",
    "from qkeras.utils import *\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "tf.keras.backend.floatx()\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize, precision=128, suppress=True)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU') == []:\n",
    "    print(\"No GPU available\")\n",
    "else:\n",
    "    print(\"GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.experimental.numpy.random.seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def get_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape + (1,)).astype(\"float32\")\n",
    "    x_test = x_test.reshape(x_test.shape + (1,)).astype(\"float32\")\n",
    "\n",
    "    x_train /= 256.0\n",
    "    x_test /= 256.0\n",
    "\n",
    "    x_mean = np.mean(x_train, axis=0)\n",
    "\n",
    "    x_train -= x_mean\n",
    "    x_test -= x_mean\n",
    "\n",
    "    nb_classes = np.max(y_train)+1\n",
    "    y_train = to_categorical(y_train, nb_classes)\n",
    "    y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "input_width = 28\n",
    "input_channels = 1\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = get_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeLayer(Layer):\n",
    "\n",
    "    \"\"\"Subclass of Layer to create an Identity or Fake layer that does not exist for TensorFlow 2.4.0:\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/Identity\"\"\"\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "        super(FakeLayer, self).__init__(name=name)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_evaluate(predictions, y):\n",
    "    index_pred = np.argmax(predictions)\n",
    "    #print(index_pred)\n",
    "    index_gold = np.argmax(y)\n",
    "    #print(index_gold)\n",
    "    if index_pred != index_gold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 # 1 when correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_padding(inputs, kernel_size, strides, padding):\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2\n",
    "    \n",
    "    in_height = inputs.shape[1]\n",
    "    in_width = inputs.shape[2]\n",
    "    filter_height = kernel_size[0]\n",
    "    filter_width = kernel_size[1]\n",
    "    stride_height = strides[0]\n",
    "    stride_width = strides[1]\n",
    "\n",
    "    if padding == \"valid\":\n",
    "        output_height = math.ceil((in_height - filter_height + 1) / stride_height)\n",
    "        output_width  = math.ceil((in_width - filter_width + 1) / stride_width)\n",
    "\n",
    "        pad_top = 0\n",
    "        pad_bottom = 0\n",
    "        pad_left = 0\n",
    "        pad_right = 0\n",
    "\n",
    "    elif padding == \"same\":\n",
    "        output_height = math.ceil(in_height / stride_height)\n",
    "        output_width  = math.ceil(in_width / stride_width)\n",
    "\n",
    "        if (in_height % stride_height == 0):\n",
    "            pad_along_height = max(filter_height - stride_height, 0)\n",
    "        else:\n",
    "            pad_along_height = max(filter_height - (in_height % stride_height), 0)\n",
    "        if (in_width % stride_width == 0):\n",
    "            pad_along_width = max(filter_width - stride_width, 0)\n",
    "        else:\n",
    "            pad_along_width = max(filter_width - (in_width % stride_width), 0)\n",
    "\n",
    "        pad_top = pad_along_height // 2\n",
    "        pad_bottom = pad_along_height - pad_top\n",
    "        pad_left = pad_along_width // 2\n",
    "        pad_right = pad_along_width - pad_left\n",
    "\n",
    "    return output_height, output_width, pad_top, pad_bottom, pad_left, pad_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class=\"anchor\" id=\"ch1\"></a>\n",
    "# 1) Quantized network design\n",
    "\n",
    "Go to next: [Ch. 2](#ch2).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top).\n",
    "\n",
    "The goal of this chapter is to design a Convolutional network that provides quantized inputs and weights to its 2D-Conv kernels, as an example.\n",
    "\n",
    "It is important to know that inside the quantized kernels of QKeras (```QConv2D```, ```QDepthwiseConv2D```, ```QDense```) there are the corresponding TensorFlow kernels (```tf.keras.backend.conv2d()```, ```tf.keras.backend.depthwise_conv2d()```, ```tf.keras.backend.dot()```) which are floating-point kernels. \n",
    "\n",
    "When running one of these quantized kernels, QKeras partially uses the technique called [\"fake quantization\"](https://github.com/google/qkeras/issues/96#issuecomment-1210877800) that is the same technique used by Tensorflow Lite [2]. This technique consists in quantizing and dequantizing inputs and weights before running the floating-point kernel. In this way, inputs, weights (and then outputs) remain floating point numbers, but can represent quantized values only. However, there is a difference: QKeras does not fake-quantize the inputs, i.e. they remain \"true\" floating point numbers so they can represent any number in the floating point range (you can look at the source code of ```QConv2D()``` in [qconvolutional.py#L294](https://github.com/google/qkeras/blob/eb6e0dc86c43128c6708988d9cb54d1e106685a4/qkeras/qconvolutional.py#L294) yourself). The same holds also for the outputs: they remain in floating point because computing a kernel with floating-point inputs and fake-quantized weights gives floating-point outputs.\n",
    "\n",
    "To tackle this problem, we perform a ```quantized_bits()``` operation on the input feature map tensor, by inserting a ```QActivation``` layer. ```quantized_bits()``` performs a quantization-dequantization (q-deq) operation on the floating point tensor. Thanks to this ```QActivation``` layer, we can extract the quantization parameters of the input feature map tensor and quantize it to integer values (see [Chapter 2](#ch2)). Now the output tensor of ```QConv2D``` is fake-quantized completely because both inputs and weights to this layer are fake-quantized.\n",
    "\n",
    "The next two layers are a standard ```ReLU``` followed by another ```QActivation``` with ```quantized_bits()```. We could have used ```QActivation(\"quantized_relu(bits,integer)\")```, but ```quantized_relu()``` does not quantize the input data in the same way as ```quantized_bits()```. In particular, the argument ```alpha=\"auto\"``` is not present in ```quantized_relu()```, so it does not quantize with the standard affine quantization mapping formula [2][3] (shown below) which is the quantization we want to implement.\n",
    "$$x_q = \\text{clip}\\Big( \\text{round}\\big(\\frac{1}{s} x + z\\big), \\alpha_q, \\beta_q \\Big)$$\n",
    "Thus, in order to tell QKeras to use this quantization mapping formula, we have to set ```alpha=\"auto\"``` in ```quantized_bits()``` for all QKeras layers.\n",
    "\n",
    "The only drawback of using ```quantized_bits()``` is that it implements only a symmetric quantized range when ```alpha=\"auto\"``` (as written in the comment [quantizers.py#L1404](https://github.com/google/qkeras/blob/c5051b51ac5d8db7b5d235419a1538258a35a8a7/qkeras/quantizers.py#L1404)), so even if we set ```symmetric=0``` and ```keep_negative=0```, it automatically forces ```symmetric=1``` ([quantizers.py#L524](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L524)) and ```keep_negative=1``` ([quantizers.py#L584](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L524), [quantizers.py#L603](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L524)). Thefore, as an example, ```quantized_bits(4,4,0,0,alpha='auto')``` will be treated by QKeras as ```quantized_bits(4,4,1,1,alpha='auto')```. \n",
    "This implies that our features will lose 1 bit in the positive range after passing a ReLU activation.\n",
    "\n",
    "Finally, the ```QActivation``` layer that follows the ```ReLU``` can be used to fake-quantize the features to another bitwidth precision. ```ReLU``` can NOT be passed to ```QConv2D``` in the ```activation``` argument because it would be threated as ```quantized_relu()```.\n",
    "\n",
    "Regarding the number of bits for ```quantized_bits()```, we want that ```bits``` = ```integer``` because our target is to implement integer-only arithmetic. \n",
    "\n",
    "<br><br>\n",
    "STRANGE THINGS.\n",
    "1) Using ```bits``` > 31 quantizes things with ```nan```. Why? Future work\n",
    "\n",
    "2) Always explicit the value of the keyword argument ```alpha``` of  ```quantized_bits()```, that is never leave the field blank, to avoid [strange behaviors](https://github.com/google/qkeras/issues/60)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POOL PARAMS\n",
    "pool_size_list = [(4, 4)]\n",
    "\n",
    "# 2DCONV PARAMS\n",
    "filters_list = [2, 3]\n",
    "kernel_size_list = [(3, 3), (3, 3)]\n",
    "strides_list = [(1, 1), (2, 2)]\n",
    "pads_list = [\"valid\", \"same\"] # \"valid\" or \"same\"\n",
    "\n",
    "# DENSE PARAMS\n",
    "units_list = [10]\n",
    "\n",
    "# QUANTIZATION PARAMS\n",
    "bit_flat = 16\n",
    "\n",
    "bits_qactiv_list  = [bit_flat, bit_flat, bit_flat, bit_flat]\n",
    "bits_qweight_list = [bit_flat, bit_flat, bit_flat, 0       ] # last value is dummy and is needed by the next for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Keras model that we want to convert in QKeras"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    MaxPooling2D(pool_size=pool_size_list[0], name=\"pool\"),\n",
    "    \n",
    "    Conv2D(filters_list[0], kernel_size_list[0], strides=strides_list[0], padding=pads_list[0], name=\"conv2d_0\"),\n",
    "    ReLU(name=\"relu_0\"),\n",
    "    \n",
    "    Conv2D(filters_list[1], kernel_size_list[1], strides=strides_list[1], padding=pads_list[1], name=\"conv2d_1\"),\n",
    "    BatchNormalization(name=\"bn_1\"),\n",
    "    ReLU(name=\"relu_1\"),\n",
    "    \n",
    "    Flatten(name=\"flatten\"),\n",
    "    \n",
    "    Dense(units_list[0], name=\"dense\"),\n",
    "    \n",
    "    Activation(\"softmax\", name=\"softmax\")\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_batchnorm = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case without fused BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually modified Keras model to prepare it for transfer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_batchnorm == 0:\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        MaxPooling2D(pool_size=pool_size_list[0], name=\"pool\"),\n",
    "        FakeLayer(name=\"act_0\"), # fake relu\n",
    "\n",
    "        # Example of Conv2D without Batchnorm\n",
    "        Conv2D(filters=filters_list[0], kernel_size=kernel_size_list[0], strides=strides_list[0], padding=pads_list[0], name=\"conv2d_0\"),\n",
    "              #activation=\"relu\"),\n",
    "        ReLU(name=\"relu_0\"), # real relu\n",
    "        FakeLayer(name=\"act_1\"), # fake relu\n",
    "\n",
    "        # Example of Conv2D with external Batchnorm\n",
    "        Conv2D(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1], name=\"conv2d_1\"),\n",
    "        BatchNormalization(name=\"bn_1\"),\n",
    "        ReLU(name=\"relu_1\"), # real relu\n",
    "        FakeLayer(name=\"act_2\"), # fake relu\n",
    "\n",
    "        Flatten(name=\"flatten\"),\n",
    "\n",
    "        # Example of Dense without Batchnorm\n",
    "        Dense(units_list[0], name=\"dense\"),\n",
    "        FakeLayer(name=\"act_3\"), # fake relu\n",
    "\n",
    "        Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    model.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "    model.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'],\n",
    "                   run_eagerly=True)\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QKeras model with new activation layer \"quantized_bits_featuremap\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_batchnorm == 0:\n",
    "    \n",
    "    qmodel = tf.keras.models.Sequential([\n",
    "    \n",
    "        MaxPooling2D(pool_size=pool_size_list[0], name=\"pool\"),\n",
    "        QActivation(\"quantized_bits_featuremap(bits=%s,integer=%s,symmetric=1,keep_negative=1,alpha='auto',scale_axis=0)\" % (bits_qactiv_list[0], bits_qactiv_list[0]), name=\"act_0\"),\n",
    "\n",
    "        # Example of Conv2D without Batchnorm\n",
    "        QConv2D(filters=filters_list[0], kernel_size=kernel_size_list[0], strides=strides_list[0], padding=pads_list[0], name=\"conv2d_0\",\n",
    "              kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % (bits_qweight_list[0], bits_qweight_list[0]), \n",
    "              bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\"),\n",
    "              #activation=\"relu\"), # This way applies quantized_relu() that we do not want\n",
    "        ReLU(name=\"relu_0\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % (bits_qactiv_list[1], bits_qactiv_list[1]), name=\"act_1\"),\n",
    "\n",
    "        # Example of Conv2D with Batchnorm\n",
    "        #QConv2DBatchnorm(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1], name=\"conv2d_1\",\n",
    "        #      kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % (bits_qweight_list[1], bits_qweight_list[1]), \n",
    "        #      bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\"),\n",
    "        #FakeLayer(name=\"bn_1\"),\n",
    "        QConv2D(filters=filters_list[1], kernel_size=kernel_size_list[1], strides=strides_list[1], padding=pads_list[1], name=\"conv2d_1\",\n",
    "              kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % (bits_qweight_list[1], bits_qweight_list[1]), \n",
    "              bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\"),\n",
    "        BatchNormalization(name=\"bn_1\"),\n",
    "        ReLU(name=\"relu_1\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % (bits_qactiv_list[2], bits_qactiv_list[2]), name=\"act_2\"),\n",
    "\n",
    "        Flatten(name=\"flatten\"),\n",
    "\n",
    "        # Example of Dense without Batchnorm\n",
    "        QDense(units_list[0],\n",
    "             kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % (bits_qweight_list[2], bits_qweight_list[2]),\n",
    "             bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\",\n",
    "             name=\"dense\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % (bits_qactiv_list[3], bits_qactiv_list[3]), name=\"act_3\"),\n",
    "\n",
    "        Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    qmodel.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "    qmodel.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'],\n",
    "                   run_eagerly=True)\n",
    "\n",
    "    qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case with fused BN (BatchNormalization --> FakeLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually modified Keras model to prepare it for transfer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pool (MaxPooling2D)          (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "act_0 (FakeLayer)            (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_0 (Conv2D)            (None, 5, 5, 2)           20        \n",
      "_________________________________________________________________\n",
      "relu_0 (ReLU)                (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "act_1 (FakeLayer)            (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 3, 3)           57        \n",
      "_________________________________________________________________\n",
      "bn_1 (FakeLayer)             (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "relu_1 (ReLU)                (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "act_2 (FakeLayer)            (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                280       \n",
      "_________________________________________________________________\n",
      "act_3 (FakeLayer)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 357\n",
      "Trainable params: 357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 17:39:58.103613: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "if use_batchnorm == 1:\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        MaxPooling2D(pool_size=pool_size_list[0], name=\"pool\"),\n",
    "        FakeLayer(name=\"act_0\"), # fake relu\n",
    "\n",
    "        # Example of Conv2D without Batchnorm\n",
    "        Conv2D(filters=filters_list[0], kernel_size=kernel_size_list[0], strides=strides_list[0], padding=pads_list[0], name=\"conv2d_0\"),\n",
    "              #activation=\"relu\"),\n",
    "        ReLU(name=\"relu_0\"), # real relu\n",
    "        FakeLayer(name=\"act_1\"), # fake relu\n",
    "\n",
    "        # Example of Conv2D with external Batchnorm\n",
    "        Conv2D(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1], name=\"conv2d_1\"),\n",
    "        FakeLayer(name=\"bn_1\"),\n",
    "        ReLU(name=\"relu_1\"), # real relu\n",
    "        FakeLayer(name=\"act_2\"), # fake relu\n",
    "\n",
    "        Flatten(name=\"flatten\"),\n",
    "\n",
    "        # Example of Dense without Batchnorm\n",
    "        Dense(units_list[0], name=\"dense\"),\n",
    "        FakeLayer(name=\"act_3\"), # fake relu\n",
    "\n",
    "        Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    model.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "    model.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'],\n",
    "                   run_eagerly=True)\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QKeras model with new activation layer \"quantized_bits_featuremap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pool (MaxPooling2D)          (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "act_0 (QActivation)          (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_0 (QConv2D)           (None, 5, 5, 2)           20        \n",
      "_________________________________________________________________\n",
      "relu_0 (ReLU)                (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "act_1 (QActivation)          (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (QConv2DBatchnorm)  (None, 3, 3, 3)           70        \n",
      "_________________________________________________________________\n",
      "bn_1 (FakeLayer)             (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "relu_1 (ReLU)                (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "act_2 (QActivation)          (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense (QDense)               (None, 10)                280       \n",
      "_________________________________________________________________\n",
      "act_3 (QActivation)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 370\n",
      "Trainable params: 363\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if use_batchnorm == 1:\n",
    "    \n",
    "    qmodel = tf.keras.models.Sequential([\n",
    "\n",
    "        MaxPooling2D(pool_size=pool_size_list[0], name=\"pool\"),\n",
    "        QActivation(\"quantized_bits_featuremap(bits=%s,integer=%s,symmetric=1,keep_negative=1,alpha='auto',scale_axis=0)\" % (bits_qactiv_list[0], bits_qactiv_list[0]), name=\"act_0\"),\n",
    "\n",
    "        # Example of Conv2D without Batchnorm\n",
    "        QConv2D(filters=filters_list[0], kernel_size=kernel_size_list[0], strides=strides_list[0], padding=pads_list[0], name=\"conv2d_0\",\n",
    "              kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % (bits_qweight_list[0], bits_qweight_list[0]), \n",
    "              bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\"),\n",
    "              #activation=\"relu\"), # This way applies quantized_relu() that we do not want\n",
    "        ReLU(name=\"relu_0\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % (bits_qactiv_list[1], bits_qactiv_list[1]), name=\"act_1\"),\n",
    "\n",
    "        # Example of Conv2D with fused Batchnorm\n",
    "        QConv2DBatchnorm(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1], name=\"conv2d_1\",\n",
    "              kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % (bits_qweight_list[1], bits_qweight_list[1]), \n",
    "              bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\"),\n",
    "        FakeLayer(name=\"bn_1\"),\n",
    "        #QConv2D(filters=filters_list[1], kernel_size=kernel_size_list[1], strides=strides_list[1], padding=pads_list[1], name=\"conv2d_1\",\n",
    "        #      kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % (bits_qweight_list[1], bits_qweight_list[1]), \n",
    "        #      bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\"),\n",
    "        #BatchNormalization(name=\"bn_1\"),\n",
    "        ReLU(name=\"relu_1\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % (bits_qactiv_list[2], bits_qactiv_list[2]), name=\"act_2\"),\n",
    "\n",
    "        Flatten(name=\"flatten\"),\n",
    "\n",
    "        # Example of Dense without Batchnorm\n",
    "        QDense(units_list[0],\n",
    "             kernel_quantizer=\"quantized_bits(%s,%s,1,1,alpha='auto')\" % (bits_qweight_list[2], bits_qweight_list[2]),\n",
    "             bias_quantizer=\"quantized_bits(31,31,1,1,alpha='auto')\",\n",
    "             name=\"dense\"),\n",
    "        QActivation(\"quantized_bits_featuremap(%s,%s,1,1,alpha='auto',scale_axis=0)\" % (bits_qactiv_list[3], bits_qactiv_list[3]), name=\"act_3\"),\n",
    "\n",
    "        Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "    ])\n",
    "\n",
    "    qmodel.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "    qmodel.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'],\n",
    "                   run_eagerly=True)\n",
    "\n",
    "    qmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save qmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = 1\n",
    "epochs = 30\n",
    "\n",
    "save_model  = 1\n",
    "save_path = \"./qmodel\"\n",
    "#save_path = \"./qmodel.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\r",
      " 1/88 [..............................] - ETA: 11s - loss: 2.8202 - accuracy: 0.1465"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 17:40:01.746510: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 9s 100ms/step - loss: 2.6153 - accuracy: 0.1561 - val_loss: 2.2481 - val_accuracy: 0.1903\n",
      "Epoch 2/30\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 2.0678 - accuracy: 0.3209 - val_loss: 1.9308 - val_accuracy: 0.3855\n",
      "Epoch 3/30\n",
      "88/88 [==============================] - 7s 83ms/step - loss: 1.7727 - accuracy: 0.4414 - val_loss: 1.6049 - val_accuracy: 0.5102\n",
      "Epoch 4/30\n",
      "88/88 [==============================] - 7s 82ms/step - loss: 1.5222 - accuracy: 0.5323 - val_loss: 1.3607 - val_accuracy: 0.5942\n",
      "Epoch 5/30\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 1.3359 - accuracy: 0.5914 - val_loss: 1.1926 - val_accuracy: 0.6399\n",
      "Epoch 6/30\n",
      "88/88 [==============================] - 9s 98ms/step - loss: 1.1802 - accuracy: 0.6371 - val_loss: 1.0607 - val_accuracy: 0.6766\n",
      "Epoch 7/30\n",
      "88/88 [==============================] - 8s 91ms/step - loss: 1.0654 - accuracy: 0.6708 - val_loss: 0.9472 - val_accuracy: 0.7072\n",
      "Epoch 8/30\n",
      "88/88 [==============================] - 8s 89ms/step - loss: 0.9593 - accuracy: 0.7004 - val_loss: 0.8611 - val_accuracy: 0.7318\n",
      "Epoch 9/30\n",
      "88/88 [==============================] - 8s 88ms/step - loss: 0.8729 - accuracy: 0.7286 - val_loss: 0.8012 - val_accuracy: 0.7483\n",
      "Epoch 10/30\n",
      "88/88 [==============================] - 7s 85ms/step - loss: 0.8257 - accuracy: 0.7409 - val_loss: 0.7553 - val_accuracy: 0.7606\n",
      "Epoch 11/30\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 0.7843 - accuracy: 0.7508 - val_loss: 0.7232 - val_accuracy: 0.7676\n",
      "Epoch 12/30\n",
      "88/88 [==============================] - 8s 85ms/step - loss: 0.7480 - accuracy: 0.7625 - val_loss: 0.6977 - val_accuracy: 0.7775\n",
      "Epoch 13/30\n",
      "88/88 [==============================] - 8s 87ms/step - loss: 0.7179 - accuracy: 0.7712 - val_loss: 0.6792 - val_accuracy: 0.7833\n",
      "Epoch 14/30\n",
      "88/88 [==============================] - 8s 89ms/step - loss: 0.6972 - accuracy: 0.7766 - val_loss: 0.6623 - val_accuracy: 0.7872\n",
      "Epoch 15/30\n",
      "88/88 [==============================] - 7s 82ms/step - loss: 0.6871 - accuracy: 0.7783 - val_loss: 0.6494 - val_accuracy: 0.7910\n",
      "Epoch 16/30\n",
      "88/88 [==============================] - 7s 84ms/step - loss: 0.6802 - accuracy: 0.7795 - val_loss: 0.6396 - val_accuracy: 0.7921\n",
      "Epoch 17/30\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 0.6661 - accuracy: 0.7859 - val_loss: 0.6290 - val_accuracy: 0.7961\n",
      "Epoch 18/30\n",
      "88/88 [==============================] - 8s 90ms/step - loss: 0.6603 - accuracy: 0.7855 - val_loss: 0.6204 - val_accuracy: 0.7989\n",
      "Epoch 19/30\n",
      "88/88 [==============================] - 7s 83ms/step - loss: 0.6466 - accuracy: 0.7891 - val_loss: 0.6134 - val_accuracy: 0.7995\n",
      "Epoch 20/30\n",
      "88/88 [==============================] - 8s 89ms/step - loss: 0.6366 - accuracy: 0.7915 - val_loss: 0.6072 - val_accuracy: 0.8032\n",
      "Epoch 21/30\n",
      "88/88 [==============================] - 8s 88ms/step - loss: 0.6249 - accuracy: 0.7971 - val_loss: 0.5997 - val_accuracy: 0.8047\n",
      "Epoch 22/30\n",
      "88/88 [==============================] - 8s 94ms/step - loss: 0.6240 - accuracy: 0.7954 - val_loss: 0.5945 - val_accuracy: 0.8080\n",
      "Epoch 23/30\n",
      "88/88 [==============================] - 8s 88ms/step - loss: 0.6018 - accuracy: 0.8033 - val_loss: 0.5895 - val_accuracy: 0.8091\n",
      "Epoch 24/30\n",
      "88/88 [==============================] - 8s 89ms/step - loss: 0.6063 - accuracy: 0.8005 - val_loss: 0.5841 - val_accuracy: 0.8112\n",
      "Epoch 25/30\n",
      "88/88 [==============================] - 7s 84ms/step - loss: 0.6055 - accuracy: 0.8021 - val_loss: 0.5792 - val_accuracy: 0.8125\n",
      "Epoch 26/30\n",
      "88/88 [==============================] - 8s 88ms/step - loss: 0.5958 - accuracy: 0.8075 - val_loss: 0.5745 - val_accuracy: 0.8153\n",
      "Epoch 27/30\n",
      "88/88 [==============================] - 8s 86ms/step - loss: 0.5994 - accuracy: 0.8041 - val_loss: 0.5701 - val_accuracy: 0.8160\n",
      "Epoch 28/30\n",
      "88/88 [==============================] - 7s 84ms/step - loss: 0.5894 - accuracy: 0.8086 - val_loss: 0.5675 - val_accuracy: 0.8158\n",
      "Epoch 29/30\n",
      "88/88 [==============================] - 7s 83ms/step - loss: 0.5840 - accuracy: 0.8097 - val_loss: 0.5643 - val_accuracy: 0.8179\n",
      "Epoch 30/30\n",
      "88/88 [==============================] - 7s 83ms/step - loss: 0.5806 - accuracy: 0.8111 - val_loss: 0.5600 - val_accuracy: 0.8177\n",
      "all files deleted\n",
      "Model saved correctly\n"
     ]
    }
   ],
   "source": [
    "if train_model == 1:\n",
    "    \n",
    "    qmodel.fit(x_train, y_train, batch_size=512,\n",
    "               epochs=epochs, validation_split=0.25, shuffle=True)    \n",
    "    \n",
    "    if save_model == 1:\n",
    "        \n",
    "        try:\n",
    "            os.remove(\"./qmodel.index\")\n",
    "            os.remove(\"./qmodel.data-00000-of-00001\")\n",
    "            os.remove(\"./checkpoint\")\n",
    "            print(\"all files deleted\")\n",
    "        except:\n",
    "            print(\"nothing to delete\")\n",
    "\n",
    "        qmodel.save_weights(save_path, overwrite=True, save_format=\"tf\")\n",
    "        #qmodel.save(save_path)\n",
    "        #os.remove(\"./checkpoint\")\n",
    "\n",
    "        print(\"Model saved correctly\")\n",
    "else:\n",
    "    qmodel.load_weights(save_path, by_name=False).expect_partial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer weights from qmodel to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for qlayer, layer in zip(qmodel.layers, model.layers):\n",
    "    \n",
    "    with io.capture_output(stdout=True, stderr=False) as captured: # to disable all the printings to stdout of the functions inside this statement: https://stackoverflow.com/questions/23610585/ipython-notebook-avoid-printing-within-a-function/23611571#23611571\n",
    "        \n",
    "        print(qlayer.__class__.__name__)\n",
    "\n",
    "        if qlayer.get_weights() and qlayer.__class__.__name__ not in [\"BatchNormalization\"]:\n",
    "            \n",
    "            print(qlayer.name)\n",
    "            print(\"qlayer.get_weights()[0].shape:\", qlayer.get_weights()[0].shape)\n",
    "            #print(len(qlayer.get_weights()))\n",
    "            #print(len(qlayer.get_weights()[0:2]))\n",
    "            try:\n",
    "                print(len(qlayer.get_folded_weights())) # non sono quantizzati da quantized_bits\n",
    "                print(\"This layer IS FOLDED\")\n",
    "            except:\n",
    "                print(\"This layer is NOT folded\")\n",
    "            #print(len(qlayer.weights))\n",
    "            \n",
    "            print(\"layer.get_weights()[0][0] MODEL BEFORE\")\n",
    "            print(layer.get_weights()[0][0])\n",
    "            print(\"layer.get_weights()[1] MODEL BEFORE\")\n",
    "            print(layer.get_weights()[1])\n",
    "            layer.set_weights(copy.deepcopy(qlayer.get_weights()[0:2]))\n",
    "            print(\"layer.get_weights()[0][0] MODEL AFTER\")\n",
    "            print(layer.get_weights()[0][0])\n",
    "            print(\"layer.get_weights()[1] MODEL AFTER\")\n",
    "            print(layer.get_weights()[1])\n",
    "            \n",
    "            print(\"qlayer.get_weights()[0][0] QMODEL\") # equal to print(\"qlayer.weights[0][0]\")\n",
    "            print(qlayer.get_weights()[0][0])\n",
    "            print(\"qlayer.get_weights()[1] QMODEL\")\n",
    "            print(qlayer.get_weights()[1])\n",
    "            \n",
    "            if (layer.get_weights()[0][0] != qlayer.get_weights()[0][0]).all() or \\\n",
    "               (layer.get_weights()[1] != qlayer.get_weights()[1]).all():\n",
    "                raise Exception(\"Transfer weights failed\")\n",
    "            \n",
    "\n",
    "        elif qlayer.get_weights() and qlayer.__class__.__name__ in [\"BatchNormalization\"]:\n",
    "\n",
    "            print(qlayer.name)\n",
    "            #print(len(qlayer.get_weights()))\n",
    "            print(\"layer.get_weights()[0] MODEL BEFORE\")\n",
    "            print(layer.get_weights()[0])\n",
    "            print(\"layer.get_weights()[1] MODEL BEFORE\")\n",
    "            print(layer.get_weights()[1])\n",
    "            layer.set_weights(copy.deepcopy(qlayer.get_weights()))\n",
    "            print(\"layer.get_weights()[0] MODEL AFTER\")\n",
    "            print(layer.get_weights()[0])\n",
    "            print(\"layer.get_weights()[1] MODEL AFTER\")\n",
    "            print(layer.get_weights()[1])\n",
    "            \n",
    "            print(\"qlayer.get_weights()[0] QMODEL\")\n",
    "            print(qlayer.get_weights()[0])\n",
    "            print(\"qlayer.get_weights()[1] QMODEL\")\n",
    "            print(qlayer.get_weights()[1])\n",
    "            \n",
    "            if (layer.get_weights()[0] != qlayer.get_weights()[0]).all() or \\\n",
    "               (layer.get_weights()[1] != qlayer.get_weights()[1]).all():\n",
    "                raise Exception(\"Transfer weights failed\")\n",
    "\n",
    "        print(\"------------\")\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "<a class=\"anchor\" id=\"ch2\"></a>\n",
    "# 2) Extract QKeras quantization factors\n",
    "Go to next: [Ch. 3](#ch3).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top).\n",
    "\n",
    "This is the complete quantization formula for a matrix multiplication operation (valid also for an FC layer) taken from [[3]#Quantized-Matrix-Multiplication-Mathematics](#https://leimao.github.io/article/Neural-Networks-Quantization/#Quantized-Matrix-Multiplication-Mathematics) (an equivalent version is Eq.7 in [2]):\n",
    "$$\\begin{align} Y_{q,i,j} &= z_Y + \\frac{s_b}{s_Y} (b_{q, j} - z_b) + \\frac{s_X s_W}{s_Y} \\Bigg[ \\bigg( \\sum_{k=1}^{p} X_{q,i,k} W_{q, k,j} \\bigg) - \\bigg( z_W \\sum_{k=1}^{p} X_{q,i,k} \\bigg) - \\bigg( z_X \\sum_{k=1}^{p} W_{q, k,j} \\bigg) + p z_X z_W\\Bigg] \\end{align}$$\n",
    "\n",
    "There are some contributions that could be deleted if the <b>zero points</b> of weights ```z_w``` and biases ```z_b``` <b>are forced to be zero, i.e. if both the quantized range and the fake-quantized/floating-point range of weights and biases, respectively, are symmetric</b>. When this happens, affine quantization mapping is called scale quantization mapping [3]. It is relatively easy to set the quantized range to be symmetric (for example, in QKeras we just need to pass ```quantized_bits()``` with ```symmetric=1``` and ```keep_negative=1``` to both the arguments ```kernel_quantizer``` and ```bias_quantizer``` of each QKeras layer), but this is not the case for the fake-quantized/floating-point range. There are two ways to make the latter symmetric:\n",
    "\n",
    "1) during training, by constraining weight and bias tensors to a given symmetric range of values;\n",
    "\n",
    "2) during training, by using a different way to calculate the scaling factor ```s```. Instead of calculating it in the standard and more general way: \n",
    "$$\\begin{align} s &= \\frac{\\beta - \\alpha}{\\beta_q - \\alpha_q}\\end{align},$$\n",
    "it can be calculated as: \n",
    "$$\\begin{align} s &= \\frac{2 * max (abs (tensor) )}{\\beta_q - \\alpha_q}\\end{align},$$\n",
    "where ```tensor``` is the floating-point weight/bias tensor to be quantized, ```[alpha; beta]``` is the floating-point range (where in turns ```alpha``` and ```beta``` are the minimum and maximum values of the entire tensor, so there is only one scalar ```s``` for the entire tensor), ```[alphaq; betaq]``` is the quantized range (which depends on the number of bits we want to represent the quantized data). Regarding the operations, ```abs()``` calculates the absolute value of all the elements in ```tensor``` and ```max``` extracts the maximum value from each channel (so in the second formula ```s``` is an array of scaling factors). Apart from the difference related to the per-layer vs per-channel quantization, the second formula is more general because removes the constraint of searching for the minimum in the tensor and directly assumes that the floating-point range (numerator) is symmetric, even if it is not actually true, but in this way it avoids to constrain ```alpha``` and ```beta``` to be exactly equal and opposite.\n",
    "\n",
    "[TensorFlow Lite states](https://www.tensorflow.org/lite/performance/quantization_spec#symmetric_vs_asymmetric) that they are forcing the zero points to zero, but they do not show how (maybe it is necessary to look at the source code: future work). [This guy](https://stackoverflow.com/questions/69746834/tf-lite-model-force-symmetric-filter-weights-in-fully-connected-layers) tried to implement the first approach using [tf.keras.constraints](https://www.tensorflow.org/api_docs/python/tf/keras/constraints) without success; instead QKeras follows the second approach, as you can see in source code of ```quantized_bits()``` in [quantizers.py#L586](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L586).\n",
    "\n",
    "In the light of the aforementioned, in the next cells of this notebook we will extract and save to .txt files only the following quantization parameters that will be used for the inference in hardware (discussed in [Chapter 3](#ch3)):\n",
    "\n",
    "- ```fq```, ```scale_f```, ```zeropoint_f``` and ```[alphaq_f, betaq_f]``` are the quantized values, the scaling factors, the zero points and the quantized range of the corresponding output features of a QActivation layer used for the q-deq operation, respectively;\n",
    "\n",
    "- ```wq``` and ```scale_w``` are the quantized weights and their scaling factors; \n",
    "\n",
    "- ```bq``` and ```scale_b``` are the quantized biases and their scaling factors;\n",
    "\n",
    "- ```subq1``` is the third term in the squared brackets in the quantization formula above (i.e. the summation over the quantized weights multiplied by the zero point of the input features):\n",
    "$$\\bigg( z_X \\sum_{k=1}^{p} W_{q, k,j} \\bigg)$$\n",
    "\n",
    "To run the notebook without issues, you should edit the file [quantizers.py](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py) to expose the following internal variables to the external world as attributes. \"To ease this step, just follow the instructions in the readme file of this repo\":\n",
    "\n",
    "- ```m_i = K.cast_to_floatx(K.pow(2, self.integer))```;\n",
    "\n",
    "- ```alphaq = -2**(self.bits-1)+1 if self.symmetric else 0```;\n",
    "\n",
    "- ```betaq = 2**(self.bits-1)-1 if self.symmetric else (2**self.bits)-1```;\n",
    "\n",
    "- ```scale1 = (K.max(abs(x), axis=axis, keepdims=True) * 2) / levels```.\n",
    "\n",
    "In particular, we need ```m_i``` and ```scale1``` to compute a scaling factor that matches the definition of scaling factor of TensorFlow Lite [2][3]. In fact, one might imagine that the scaling factor provided by the ```scale``` attribute of ```quantized_bits()``` is the same as the TensorFlow one: unfortunately it is not, as you can see from [quantizers.py#L608](https://github.com/google/qkeras/blob/b91d8815b31f05ddf9c7b6d62381df9be72a570a/qkeras/quantizers.py#L608). The correct scaling factor is ```scale = scale1 * m_i``` and has to be computed manually.\n",
    "\n",
    "Finally, the following calculations show the <b>quantization of the weights</b>, which is a <b>per-channel</b>  approach, i.e. weights have a number of scaling factors and zero points equal to the number of output channels, while <b>activations are quantized in a per-layer fashion</b> (one scaling factor and one zero point for each feature map tensor). The difference is the use of ```scale_axis=0```in ```quantized_bits()``` for ```QActivation()```. The reason why per-channel quantization of activations is not implemented in QKeras, as well as in TensorFlow Lite, is because \"<i>per-channel quantization of activations is much harder to implement because we cannot factor the scale factor out of the summation and would, therefore, require rescaling the accumulator for each input channel</i>\" [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./generated_txtfiles_from_python/ exists\n",
      "./float/ exists\n",
      "./quantization_parameters/ exists\n",
      "./generated_txtfiles_from_python/qdense/ exists\n",
      "./generated_txtfiles_from_python/qconv2d/ exists\n",
      "./generated_txtfiles_from_python/qdepthwise/ exists\n",
      "./qreverse/ exists\n",
      "./quantized_outputs/ exists\n"
     ]
    }
   ],
   "source": [
    "folder_generated_txtfiles_from_python = \"./generated_txtfiles_from_python/\"\n",
    "\n",
    "folder = \"./float/\" # contains the feature maps of each layer in floating point\n",
    "qfolder = \"./quantization_parameters/\" # contains all the quantized feature maps and weights obtained from inference from Python (not from conversion from float)\n",
    "qfolder_qdense = folder_generated_txtfiles_from_python + \"qdense/\"\n",
    "qfolder_qconv2d = folder_generated_txtfiles_from_python + \"qconv2d/\"\n",
    "qfolder_qdepthwise = folder_generated_txtfiles_from_python + \"qdepthwise/\"\n",
    "qreverse = \"./qreverse/\"\n",
    "qoutput = \"./quantized_outputs/\" # contains all the quantized output feature maps obtained from inference from Python (not from conversion from float)\n",
    "\n",
    "folders_list = []\n",
    "folders_list.append(folder_generated_txtfiles_from_python)\n",
    "folders_list.append(folder)\n",
    "folders_list.append(qfolder)\n",
    "folders_list.append(qfolder_qdense)\n",
    "folders_list.append(qfolder_qconv2d)\n",
    "folders_list.append(qfolder_qdepthwise)\n",
    "folders_list.append(qreverse)\n",
    "folders_list.append(qoutput)\n",
    "\n",
    "for f in folders_list:\n",
    "    try:    \n",
    "        os.mkdir(f)\n",
    "    except:\n",
    "        print(f + \" exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precision_float = '%32.128f'\n",
    "precision_quantized = '%.1f'\n",
    "\n",
    "random_data = 0\n",
    "\n",
    "def generate_txtfiles_from_python(model, x_sample):\n",
    "\n",
    "    print(\"generate_txtfiles_from_python\")\n",
    "    \n",
    "    # Reset all lists\n",
    "\n",
    "    of_list = []\n",
    "    w_list = []\n",
    "    b_list = []\n",
    "\n",
    "    alphaq_of_list = []\n",
    "    #alphaq_w_list = []\n",
    "    #alphaq_b_list = []\n",
    "    betaq_of_list = []\n",
    "    #betaq_w_list = []\n",
    "    #betaq_b_list = []\n",
    "    fq_list = []\n",
    "    wq_list = []\n",
    "    bq_list = []\n",
    "\n",
    "    #zeropoint_w_list = []\n",
    "    #zeropoint_b_list = []\n",
    "    zeropoint_of_list = []\n",
    "    scale_of_list = []\n",
    "\n",
    "    scale_w_list = []\n",
    "    scale_b_list = []\n",
    "    subq1_list = []\n",
    "    #subq2_list = []\n",
    "    #sumq3_list = []\n",
    "\n",
    "    layer_index = 0\n",
    "    qdense_index = 0\n",
    "    qconv2d_index = 0\n",
    "    qdepthwise_index = 0\n",
    "\n",
    "    first_layer_flag = 0\n",
    "    ready1 = 0\n",
    "    ready2 = 0\n",
    "    \n",
    "    previous_layers = [\"None\", \"None\", \"None\"]\n",
    "\n",
    "    # print the output of the input layer (i.e. the input layer itself)\n",
    "    if random_data == 0:\n",
    "        data = np.reshape(x_sample, (1, input_width,input_width,input_channels))\n",
    "    else:\n",
    "        data = tf.random.normal((1, input_width,input_width,input_channels))\n",
    "    of_list.append(data)\n",
    "    np.savetxt(folder+str(layer_index)+\"_input.txt\", np.reshape(data, (input_width*input_width*input_channels,)), fmt=precision_float)\n",
    "    layer_index += 1\n",
    "    \n",
    "    #print(model.layers)\n",
    "\n",
    "    for layer in model.layers:\n",
    "\n",
    "        print(\"--> \", layer.name)\n",
    "        print(\"    \", layer.__class__.__name__)\n",
    "\n",
    "        # print dequantized output features\n",
    "        extractor = tf.keras.Model( inputs=model.inputs,\n",
    "                                   outputs=model.get_layer(layer.name).output)\n",
    "        of = extractor(data).numpy()\n",
    "        #\n",
    "        #print(\"of:\", of)\n",
    "        of_list.append(of)\n",
    "        fname = folder+str(layer_index)+\"_of_\"+layer.name+\".txt\"\n",
    "        np.savetxt(fname, np.reshape(of, (of.size,)), fmt=precision_float)\n",
    "\n",
    "\n",
    "        ##### WEIGHTS #####\n",
    "        if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\", \"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\", \"QDense\", \"QDenseBatchnorm\"]:\n",
    "\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QDepthwiseConv2D\", \"QDense\"]:\n",
    "                parameters = layer.weights\n",
    "            else:\n",
    "                parameters = layer.get_folded_weights() # folded weights not quantized\n",
    "\n",
    "            ##### CONV2D WEIGHTS #####\n",
    "            quantizer_w = layer.get_quantizers()[0]\n",
    "\n",
    "            # print dequantized weights\n",
    "            w = parameters[0].numpy()\n",
    "            #\n",
    "            print(\"w.shape:\", w.shape)\n",
    "            w_list.append(w)\n",
    "            fname = folder+str(layer_index)+\"_w_\"+layer.name+\".txt\"\n",
    "            np.savetxt(fname, np.reshape(w, (w.size,)), fmt=precision_float)\n",
    "            \n",
    "\n",
    "            # (not needed)\n",
    "            #if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\", \"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]: # dwconv has output channel = 1 by default in qkeras\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\"]:\n",
    "                alpha_w = w.min(axis=(0, 1, 2)) # the second axis contains the input channels, the third the output channels.\n",
    "                beta_w = w.max(axis=(0, 1, 2))\n",
    "            elif layer.__class__.__name__ in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]:\n",
    "                alpha_w = w.min(axis=(0, 1))\n",
    "                beta_w = w.max(axis=(0, 1))\n",
    "            elif layer.__class__.__name__ in [\"QDense\", \"QDenseBatchnorm\"]:\n",
    "                alpha_w = w.min(axis=0)\n",
    "                beta_w = w.max(axis=0)\n",
    "            alphaq_w = quantizer_w.alphaq\n",
    "            betaq_w = quantizer_w.betaq\n",
    "            #\n",
    "            print(\"alpha_w: \", alpha_w)\n",
    "            print(\"beta_w: \", beta_w)\n",
    "            print(\"alphaq_w: \", alphaq_w)\n",
    "            print(\"betaq_w: \", betaq_w)\n",
    "            #alphaq_w_list.append(alphaq_w)\n",
    "            #betaq_w_list.append(betaq_w)\n",
    "\n",
    "\n",
    "            # print weights scale\n",
    "            scale1_w = quantizer_w.scale1.numpy().flatten()\n",
    "            m_i = quantizer_w.m_i.numpy().flatten()\n",
    "            scale_w = scale1_w * m_i\n",
    "            #\n",
    "            print(\"axis:\", quantizer_w.axis) # for dwconv it has to be [0,1] to have a per-channel quantization of weights. This is a bug of qkeras.\n",
    "            print(\"scale_w: \", scale_w)\n",
    "            scale_w_list.append(scale_w)\n",
    "            fname = qfolder+str(layer_index)+\"_w_s_\"+layer.name+\".txt\"\n",
    "            np.savetxt(fname, scale_w, fmt=precision_float)\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\"]:\n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_w_s.txt\"\n",
    "            elif layer.__class__.__name__ in [\"QDense\", \"QDenseBatchnorm\"]:\n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_w_s.txt\"\n",
    "            elif layer.__class__.__name__ in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]:\n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_w_s.txt\"\n",
    "            np.savetxt(fname, scale_w, fmt=precision_float)\n",
    "\n",
    "            \n",
    "            # print weights zero point # (not needed)\n",
    "            #tmp = [(beta_w[i]*alphaq_w - alpha_w[i]*betaq_w)/(beta_w[i] - alpha_w[i]) if elem != 0 else 0 for i,elem in enumerate((beta_w - alpha_w))]\n",
    "            #z_w = np.trunc(tmp + np.sign(tmp)*0.5)\n",
    "            #print(\"z_w:\", z_w)\n",
    "            #zeropoint_w_list.append(z_w)\n",
    "            #fname = qfolder+str(layer_index)+\"_w_z_\"+layer.name+\".txt\"\n",
    "            #np.savetxt(fname, z_w, fmt='%3.3f')\n",
    "\n",
    "\n",
    "            # print quantized weights\n",
    "            tmp = np.divide(w, scale_w, dtype=np.float64)\n",
    "            tmp[np.isnan(tmp)] = 0\n",
    "            tmp[np.isinf(tmp)] = 0\n",
    "            wq = np.clip(np.trunc(tmp + np.sign(tmp)*0.5), alphaq_w, betaq_w)\n",
    "            #\n",
    "            #print(\"w.shape:\", w.shape)\n",
    "            #print(\"w:\", w)\n",
    "            #print(\"wq:\", wq)\n",
    "            wq_list.append(wq)\n",
    "            fname = qfolder+str(layer_index)+\"_wq_\"+layer.name+\".txt\"\n",
    "            np.savetxt(fname, np.reshape(wq, (wq.size,)), fmt=precision_quantized)\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\"]:\n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_wq.txt\"\n",
    "            elif layer.__class__.__name__ in [\"QDense\", \"QDenseBatchnorm\"]:\n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_wq.txt\"\n",
    "            elif layer.__class__.__name__ in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]:\n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_wq.txt\"\n",
    "            np.savetxt(fname, np.reshape(wq, (wq.size,)), fmt=precision_quantized)\n",
    "\n",
    "\n",
    "            # print \"subq1\" (part 2)\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\"]:\n",
    "                sum_of_weights = wq.sum(axis=(0, 1, 2))\n",
    "            elif layer.__class__.__name__ in [\"QDense\", \"QDenseBatchnorm\"]:\n",
    "                sum_of_weights = wq.sum(axis=0)\n",
    "            if layer.__class__.__name__ in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]:\n",
    "                sum_of_weights = wq.sum(axis=(0, 1))\n",
    "            #print(\"wq.shape:\", wq.shape)\n",
    "            #print(\"sum_of_weights conv: \", wq.sum(axis=(0, 1, 2)))\n",
    "            #print(\"sum_of_weights dense: \", wq.sum(axis=0))\n",
    "            #print(\"sum_of_weights dwconv: \", wq.sum(axis=(0, 1)))\n",
    "            print(\"sum_of_weights: \", sum_of_weights)\n",
    "            if ready1 == 1:\n",
    "                subq1 = (zeropoint_of_list[-1] * sum_of_weights).flatten()\n",
    "                #\n",
    "                print(\"subq1: \", subq1)\n",
    "                subq1_list.append(subq1)\n",
    "                fname = qfolder+str(layer_index)+\"_subq1_\"+layer.name+\".txt\"\n",
    "                np.savetxt(fname, np.reshape(subq1, (subq1.size,)), fmt=precision_quantized)\n",
    "                if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\"]:\n",
    "                    fname = qfolder_qconv2d+str(qconv2d_index)+\"_subq1.txt\"\n",
    "                elif layer.__class__.__name__ in [\"QDense\", \"QDenseBatchnorm\"]:\n",
    "                    fname = qfolder_qdense+str(qdense_index)+\"_subq1.txt\"\n",
    "                elif layer.__class__.__name__ in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]:\n",
    "                    fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_subq1.txt\"\n",
    "                np.savetxt(fname, np.reshape(subq1, (subq1.size,)), fmt=precision_quantized)\n",
    "                ready1 = 0\n",
    "\n",
    "                \n",
    "            # print \"subq2\" (part 2) # (not needed)\n",
    "            #if ready2 == 1:\n",
    "            #    subq2 = [z_w[0] * sum_of_ofeatures]\n",
    "            #    #\n",
    "            #    #print(\"subq2: \", subq2)\n",
    "            #    #subq2_list.append(subq2)\n",
    "            #    #fname = qfolder+str(layer_index)+\"_subq2_\"+layer.name+\".txt\"\n",
    "            #    #np.savetxt(fname, subq2, fmt='%3.3f')\n",
    "            #    ready2 = 0\n",
    "\n",
    "            # (not needed)\n",
    "            #sumq3 = [input_channels * int(np.sqrt(wq.size))**2 * z_w[0] * zeropoint_of_list[-1]]\n",
    "            #print(\"sumq3: \", sumq3)\n",
    "            #sumq3_list.append(sumq3)\n",
    "            #fname = qfolder+str(layer_index)+\"_sumq3_\"+layer.name+\".txt\"\n",
    "            #np.savetxt(fname, sumq3, fmt='%3.3f')\n",
    "\n",
    "\n",
    "\n",
    "            ##### BIASES ######\n",
    "            quantizer_b = layer.get_quantizers()[1]\n",
    "\n",
    "            # print biases\n",
    "            b = parameters[1].numpy()\n",
    "            #\n",
    "            #b = np.reshape(b, (b.size,))\n",
    "            b_list.append(b)\n",
    "            fname = folder+str(layer_index)+\"_b_\"+layer.name+\".txt\"\n",
    "            np.savetxt(fname, np.reshape(b, (b.size,)), fmt=precision_float)\n",
    "\n",
    "            # (not needed)\n",
    "            alpha_b = b.min(axis=0) # b is a 1D array\n",
    "            beta_b = b.max(axis=0)\n",
    "            alphaq_b = quantizer_b.alphaq\n",
    "            betaq_b = quantizer_b.betaq\n",
    "            #if alphaq_b == -7:\n",
    "            #    alphaq_b = 0\n",
    "            #if betaq_b == 15:\n",
    "            #    betaq_b = 7\n",
    "            #\n",
    "            #print(\"alpha_b: \", alpha_b)\n",
    "            #print(\"beta_b: \", beta_b)\n",
    "            #print(\"alphaq_b: \", alphaq_b)\n",
    "            #print(\"betaq_b: \", betaq_b)\n",
    "            #alphaq_b_list.append(alphaq_b)\n",
    "            #betaq_b_list.append(betaq_b)\n",
    "\n",
    "\n",
    "            # print bias scale\n",
    "            scale1_b = quantizer_b.scale1.numpy().flatten()\n",
    "            m_i = quantizer_b.m_i.numpy().flatten()\n",
    "            scale_b = scale1_b * m_i\n",
    "            #\n",
    "            print(\"scale_b: \", scale_b)\n",
    "            scale_b_list.append(scale_b)\n",
    "            fname = qfolder+str(layer_index)+\"_b_s_\"+layer.name+\".txt\"\n",
    "            np.savetxt(fname, scale_b, fmt=precision_float)\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\"]:\n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_b_s.txt\"\n",
    "            elif layer.__class__.__name__ in [\"QDense\", \"QDenseBatchnorm\"]:\n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_b_s.txt\"\n",
    "            elif layer.__class__.__name__ in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]:\n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_b_s.txt\"\n",
    "            np.savetxt(fname, scale_b, fmt=precision_float)\n",
    "\n",
    "            # print bias zero point # (not needed)\n",
    "            #if (beta_b - alpha_b) != 0:\n",
    "            #    tmp = (beta_b*alphaq_b - alpha_b*betaq_b)/(beta_b - alpha_b)\n",
    "            #    tmp[np.isnan(tmp)] = 0\n",
    "            #    tmp[np.isinf(tmp)] = 0\n",
    "            #    z_b = [np.trunc(tmp + np.sign(tmp)*0.5)]\n",
    "            #else:\n",
    "            #    z_b = [0]\n",
    "            #\n",
    "            #print(\"z_b:\", z_b)\n",
    "            #zeropoint_b_list.append(z_b)\n",
    "            #fname = qfolder+str(layer_index)+\"_b_z_\"+layer.name+\".txt\"\n",
    "            #np.savetxt(fname, z_b, fmt='%3.3f')\n",
    "\n",
    "\n",
    "            # print quantized bias\n",
    "            if scale_b != 0:\n",
    "                tmp = np.divide(b, scale_b, dtype=np.float64)\n",
    "                tmp[np.isnan(tmp)] = 0\n",
    "                tmp[np.isinf(tmp)] = 0\n",
    "                bq = np.clip(np.trunc(tmp + np.sign(tmp)*0.5), alphaq_b, betaq_b)\n",
    "            else:\n",
    "                bq = np.zeros(b.size)\n",
    "            #\n",
    "            print(\"b:\", b)\n",
    "            print(\"bq: \", bq)\n",
    "            bq_list.append(bq)\n",
    "            fname = qfolder+str(layer_index)+\"_bq_\"+layer.name+\".txt\"\n",
    "            np.savetxt(fname, bq, fmt=precision_quantized)\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\"]:\n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_bq.txt\"\n",
    "            elif layer.__class__.__name__ in [\"QDense\", \"QDenseBatchnorm\"]:\n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_bq.txt\"\n",
    "            elif layer.__class__.__name__ in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]:\n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_bq.txt\"\n",
    "            np.savetxt(fname, bq, fmt=precision_quantized)\n",
    "            \n",
    "            \n",
    "\n",
    "        ##### FEATURES #####\n",
    "        elif layer.__class__.__name__ in [\"QActivation\"]:\n",
    "\n",
    "            # axis 0 is for batch, 1 and 2 are for feature map, 3 is for channels\n",
    "            quantizer_of = layer.quantizer\n",
    "            #\n",
    "            print(\"of.shape:\", of.shape)\n",
    "            \n",
    "            alpha_of = quantizer_of.alpha_f.numpy().flatten()\n",
    "            beta_of = quantizer_of.beta_f.numpy().flatten()\n",
    "            #\n",
    "            print(\"alpha_of_new (input): \", alpha_of)\n",
    "            print(\"beta_of_new (input): \", beta_of)\n",
    "            print(\"QActivation uses quantized_bits_featuremaps()\")\n",
    "                \n",
    "            alphaq_of = quantizer_of.alphaq\n",
    "            betaq_of = quantizer_of.betaq\n",
    "            # \n",
    "            print(\"alphaq_of: \", alphaq_of)\n",
    "            print(\"betaq_of: \", betaq_of)\n",
    "            alphaq_of_list.append(alphaq_of)\n",
    "            betaq_of_list.append(betaq_of)\n",
    "           \n",
    "            # print features scale (per-layer)\n",
    "            # is equal to the scale factor calculated inside quantized_bits() when scale_axis=0.\n",
    "            # TODO: da sostituire con valore calcolato con il calibration set per ogni layer di features\n",
    "            #levels = quantizer_of.levels\n",
    "            #scale_of = [(2 * abs(of_list[-2]).max(axis=None)) / levels]\n",
    "            #print(\"levels:\", levels)\n",
    "            #print(\"abs(of_list[-2]).max(axis=None):\", abs(of_list[-2]).max(axis=None))\n",
    "            #print(\"scale_of (per-layer) (output): \", scale_of)\n",
    "            #scale_of_list.append(scale_of)\n",
    "            #fname = qfolder+str(layer_index)+\"_of_s_\"+layer.name+\".txt\"\n",
    "            #np.savetxt(fname, scale_of, fmt=precision_float)\n",
    "            \n",
    "            # print features scale (per-layer)\n",
    "            # (not needed for activations, but I use it with quantized_bits(scale_axis=0))\n",
    "            scale1_of = quantizer_of.scale1.numpy().flatten()\n",
    "            #m_i = quantizer_of.m_i.numpy().flatten()\n",
    "            #scale_of2 = scale1_of * m_i\n",
    "            scale_of2 = scale1_of\n",
    "            print(\"scale_of (per-layer) (input): \", scale_of2)\n",
    "            scale_of_list.append(scale_of2)\n",
    "            fname = qfolder+str(layer_index)+\"_of_s_\"+layer.name+\".txt\"\n",
    "            np.savetxt(fname, scale_of2, fmt=precision_float)\n",
    "            \n",
    "            # print features zero point\n",
    "            #z_of = [np.around(((beta_of[i]*alphaq_of - alpha_of[i]*betaq_of)/(beta_of[i] - alpha_of[i])), 0) if elem != 0 else 0 for i,elem in enumerate((beta_of - alpha_of))]\n",
    "            #if (beta_of - alpha_of) != 0: \n",
    "            #    z_of = np.asarray([np.around(((beta_of*alphaq_of - alpha_of*betaq_of)/(beta_of - alpha_of)), 0)], dtype=np.float64)\n",
    "            #else:\n",
    "            #    z_of = [0]\n",
    "            #\n",
    "            #print(\"z_of (output):\", z_of)\n",
    "            \n",
    "            # print features zero point\n",
    "            z_of = quantizer_of.zeropoint.numpy().flatten()\n",
    "            #print(\"z_of_new (input):\", z_of)\n",
    "            z_of[np.isnan(z_of)] = 0\n",
    "            z_of[np.isinf(z_of)] = 0\n",
    "            #\n",
    "            print(\"z_of_new (input):\", z_of)\n",
    "            zeropoint_of_list.append(z_of)\n",
    "            fname = qfolder+str(layer_index)+\"_of_z_\"+layer.name+\".txt\"\n",
    "            np.savetxt(fname, z_of, fmt=precision_quantized)\n",
    "            ready1 = 1 # print \"subq1\" (part 1)\n",
    "                    \n",
    "            \n",
    "            # print quantized output features\n",
    "            # outputq_1 = np.around((zeropoint_of_list[i+1] + np.divide(output_bias, scale_of_list[i+1])), 0)\n",
    "            # outputq_2 = np.where(output_bias < 0, zeropoint_of_list[i+1], outputq_1)\n",
    "            # outputq_3 = np.asarray(np.clip(outputq_2, alphaq_of_list[i+1], betaq_of_list[i+1]), dtype=np.float64)\n",
    "            #ofq = np.clip(np.around((np.divide(of, scale_of) + z_of), 0), alphaq_of, betaq_of)\n",
    "            #ofq[np.isnan(ofq)] = 0\n",
    "            #print(\"ofq (calcolato nel notebook)\", ofq)\n",
    "            #\n",
    "            ofq = quantizer_of.outputq.numpy()\n",
    "            #print(\"of (input a questo layer):\", of_list[-2])\n",
    "            #print(\"of (input a questo layer interno) inputx:\", quantizer_of.inputx.numpy())\n",
    "            #print(\"outputq before_rounding (interno):\", quantizer_of.before_rounding.numpy())\n",
    "            #print(\"outputq: (calcolato nella nuova classe)\", ofq)\n",
    "            #print(\"of (q-dq) (output interno esterno):\", of)\n",
    "            #print(\"ofq (outputq esterno):\", ofq)\n",
    "            fq_list.append(ofq)\n",
    "            fname = qfolder+str(layer_index)+\"_ofq_\"+layer.name+\".txt\"\n",
    "            np.savetxt(fname, ofq.flatten(), fmt=precision_quantized)\n",
    "            \n",
    "            idx_pl = -2 # index previous layer\n",
    "            \n",
    "            if previous_layers[idx_pl] in [\"QDense\"] or previous_layers[idx_pl+1] in [\"QDense\"]:\n",
    "                \n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_of_s_in.txt\"\n",
    "                np.savetxt(fname, scale_of_list[-2], fmt=precision_float)\n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_of_s_out.txt\"\n",
    "                np.savetxt(fname, scale_of_list[-1], fmt=precision_float)\n",
    "                \n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_of_z_in.txt\"\n",
    "                np.savetxt(fname, zeropoint_of_list[-2], fmt=precision_quantized)\n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_of_z_out.txt\"\n",
    "                np.savetxt(fname, zeropoint_of_list[-1], fmt=precision_quantized)\n",
    "                \n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_ofq_in.txt\"\n",
    "                np.savetxt(fname, fq_list[-2].flatten(), fmt=precision_quantized)\n",
    "                fname = qfolder_qdense+str(qdense_index)+\"_ofq_out.txt\"\n",
    "                np.savetxt(fname, fq_list[-1].flatten(), fmt=precision_quantized)\n",
    "                \n",
    "                # If this is the first layer\n",
    "                if first_layer_flag == 0 and qdense_index == 0: \n",
    "                    fname = qfolder_qdense+str(qdense_index)+\"_ofq_in_out.txt\"\n",
    "                    np.savetxt(fname, fq_list[-2].flatten(), fmt=precision_quantized)\n",
    "                    first_layer_flag = 1\n",
    "                    \n",
    "                qdense_index += 1\n",
    "                \n",
    "            elif previous_layers[idx_pl] in [\"QConv2D\", \"QConv2DBatchnorm\"] or previous_layers[idx_pl-1] in [\"QConv2DBatchnorm\"]:\n",
    "                \n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_of_s_in.txt\"\n",
    "                np.savetxt(fname, scale_of_list[-2], fmt=precision_float)\n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_of_s_out.txt\"\n",
    "                np.savetxt(fname, scale_of_list[-1], fmt=precision_float)\n",
    "                \n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_of_z_in.txt\"\n",
    "                np.savetxt(fname, zeropoint_of_list[-2], fmt=precision_quantized)\n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_of_z_out.txt\"\n",
    "                np.savetxt(fname, zeropoint_of_list[-1], fmt=precision_quantized)\n",
    "                \n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_ofq_in.txt\"\n",
    "                np.savetxt(fname, fq_list[-2].flatten(), fmt=precision_quantized)\n",
    "                fname = qfolder_qconv2d+str(qconv2d_index)+\"_ofq_out.txt\"\n",
    "                np.savetxt(fname, fq_list[-1].flatten(), fmt=precision_quantized)\n",
    "                \n",
    "                # If this is the first layer\n",
    "                if first_layer_flag == 0 and qconv2d_index == 0: \n",
    "                    fname = qfolder_qconv2d+str(qconv2d_index)+\"_ofq_in_out.txt\"\n",
    "                    np.savetxt(fname, fq_list[-2].flatten(), fmt=precision_quantized)\n",
    "                    first_layer_flag = 1\n",
    "\n",
    "                qconv2d_index += 1\n",
    "                    \n",
    "            elif previous_layers[idx_pl] in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"] or previous_layers[idx_pl-1] in [\"QDepthwiseConv2DBatchnorm\"]:\n",
    "                \n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_of_s_in.txt\"\n",
    "                np.savetxt(fname, scale_of_list[-2], fmt=precision_float)\n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_of_s_out.txt\"\n",
    "                np.savetxt(fname, scale_of_list[-1], fmt=precision_float)\n",
    "                \n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_of_z_in.txt\"\n",
    "                np.savetxt(fname, zeropoint_of_list[-2], fmt=precision_quantized)\n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_of_z_out.txt\"\n",
    "                np.savetxt(fname, zeropoint_of_list[-1], fmt=precision_quantized)\n",
    "                \n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_ofq_in.txt\"\n",
    "                np.savetxt(fname, fq_list[-2].flatten(), fmt=precision_quantized)\n",
    "                fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_ofq_out.txt\"\n",
    "                np.savetxt(fname, fq_list[-1].flatten(), fmt=precision_quantized)\n",
    "                \n",
    "                # If this is the first layer\n",
    "                if first_layer_flag == 0 and qdepthwise_index == 0: \n",
    "                    fname = qfolder_qdepthwise+str(qdepthwise_index)+\"_ofq_in_out.txt\"\n",
    "                    np.savetxt(fname, fq_list[-2].flatten(), fmt=precision_quantized)\n",
    "                    first_layer_flag = 1\n",
    "                    \n",
    "                qdepthwise_index += 1\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                print(\"previous_layers[idx_pl]:\", previous_layers[idx_pl])\n",
    "\n",
    "\n",
    "            # print \"subq2\" # (not needed)\n",
    "            #try:\n",
    "            #    sum_of_ofeatures = ofq.sum(axis=None)\n",
    "            #    #print(\"sum_of_ofeatures: \", sum_of_ofeatures)\n",
    "            #    ready2 = 1 # print \"subq2\" (part 1)\n",
    "            #except: # for the last qdense layer\n",
    "            #    print(\"sum_of_ofeatures: N.D. for last dense layer\")      \n",
    "                \n",
    "        layer_index += 1\n",
    "        previous_layers.append(layer.__class__.__name__)\n",
    "        \n",
    "    \n",
    "    print(\"------------------------\\n\")\n",
    "    return of_list,w_list,b_list,alphaq_of_list,betaq_of_list,fq_list,wq_list,bq_list,zeropoint_of_list,scale_of_list,scale_w_list,scale_b_list,subq1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class=\"anchor\" id=\"ch3\"></a>\n",
    "# 3) Run inference and compare model with qmodel\n",
    "\n",
    "Go to next: [Ch. 4](#ch4).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_layers: 3\n",
      "-------------------------\n",
      "TOT iterations:         100\n",
      "TOT test_acc_model:     0.39\n",
      "TOT test_acc_qmodel:    0.91\n"
     ]
    }
   ],
   "source": [
    "offset = 0\n",
    "samples_to_run = 100\n",
    "min_samples = offset\n",
    "max_samples = offset+samples_to_run\n",
    "\n",
    "print_prediction = False\n",
    "\n",
    "# number of layers (with weights) of the networks\n",
    "tot_layers = 0\n",
    "for layer in qmodel.layers:\n",
    "    if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\", \"QDense\", \"QDepthwiseConv2D\"]:\n",
    "        tot_layers += 1\n",
    "print(\"tot_layers:\", tot_layers)\n",
    "\n",
    "test_acc_accumulator_model = 0\n",
    "test_acc_accumulator_qmodel = 0\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "for x, y in zip(x_test[min_samples:max_samples], y_test[min_samples:max_samples]):\n",
    "        \n",
    "    # uncomment the next line and indent the next two to disable all the printings to stdout of those two functions\n",
    "    # https://stackoverflow.com/questions/23610585/ipython-notebook-avoid-printing-within-a-function/23611571#23611571\n",
    "    #with io.capture_output(stdout=True, stderr=False) as captured:\n",
    "    #    of_list,w_list,b_list,alphaq_of_list,betaq_of_list,fq_list,wq_list,bq_list,zeropoint_of_list,scale_of_list,scale_w_list,scale_b_list,subq1_list = generate_txtfiles_from_python(qmodel, x)\n",
    "       \n",
    "    x_reshaped = x.reshape(1, x.shape[0], x.shape[1], x.shape[2])\n",
    "    \n",
    "    # Predict the samples with model, i.e. the Keras model\n",
    "    pred_model = np.asarray(model.predict(x_reshaped, batch_size=1, verbose=0), dtype=np.float64)\n",
    "    #print(pred_model)\n",
    "    #print(\"--> quantize keras output (round and clip)\")\n",
    "    #tmp = zeropoint_of_list[-1] + np.divide(pred_model, scale_of_list[-1], dtype=np.float64)\n",
    "    #tmp[np.isnan(tmp)] = 0\n",
    "    #tmp[np.isinf(tmp)] = 0\n",
    "    #pred_model_q1 = np.trunc(tmp + np.sign(tmp)*0.5)\n",
    "    #pred_model_q2 = np.asarray(np.clip(pred_model_q1, alphaq_of_list[-1], betaq_of_list[-1]), dtype=np.float64)\n",
    "\n",
    "    # Predict the samples with qmodel, i.e. the QKeras model\n",
    "    pred_qmodel = np.asarray(qmodel.predict(x_reshaped, batch_size=1, verbose=0), dtype=np.float64)\n",
    "    #print(\"--> quantize qkeras output (round and clip)\")\n",
    "    #tmp = zeropoint_of_list[-1] + np.divide(pred_qmodel, scale_of_list[-1], dtype=np.float64)\n",
    "    #tmp[np.isnan(tmp)] = 0\n",
    "    #tmp[np.isinf(tmp)] = 0\n",
    "    #pred_qmodel_q1 = np.trunc(tmp + np.sign(tmp)*0.5)\n",
    "    #pred_qmodel_q2 = np.asarray(np.clip(pred_qmodel_q1, alphaq_of_list[-1], betaq_of_list[-1]), dtype=np.float64)\n",
    "\n",
    "    # Calculate test accuracy\n",
    "    test_acc_model  = my_evaluate(pred_model,  y)\n",
    "    test_acc_qmodel = my_evaluate(pred_qmodel, y)\n",
    "\n",
    "    if print_prediction == True:\n",
    "        \n",
    "        print(\"\\npred_model:\\n\", pred_model)\n",
    "        #print(\"pred_model_q2:\\n\", pred_model_q2)\n",
    "\n",
    "        print(\"pred_qmodel:\\n\", pred_qmodel)\n",
    "        #print(\"pred_qmodelcxx_q2:\\n\", pred_qmodel_q2)\n",
    "        \n",
    "        print(\"test_acc_model:   \", test_acc_model)\n",
    "        print(\"test_acc_qmodel:  \", test_acc_qmodel)\n",
    "        \n",
    "    test_acc_accumulator_model  += test_acc_model\n",
    "    test_acc_accumulator_qmodel += test_acc_qmodel\n",
    "\n",
    "    iterations = iterations + 1\n",
    "    \n",
    "    if print_prediction == True:\n",
    "        print(\"iteration %d/%d\" % (iterations, (max_samples - min_samples)))\n",
    "        print(\"-------------------------\")\n",
    "    \n",
    "print(\"-------------------------\")\n",
    "\n",
    "print(\"TOT iterations:        \", iterations)\n",
    "print(\"TOT test_acc_model:    \", test_acc_accumulator_model/iterations)\n",
    "print(\"TOT test_acc_qmodel:   \", test_acc_accumulator_qmodel/iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--Go to next: [Ch. 5](#ch5).-->\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a class=\"anchor\" id=\"ch4\"></a>\n",
    "# 4) Quantized network design for AutoQKeras\n",
    "\n",
    "Go to next: [Ch. 5](#ch5).\n",
    "\n",
    "Go to others: [Ch. 0](#ch0), [Ch. 1](#ch1), [Ch. 2](#ch2), [Ch. 3](#ch3), [Ch. 4](#ch4).\n",
    "\n",
    "Go to [Top](#top).\n",
    "\n",
    "To perform an hyperparameter search on a Keras model with AutoQKeras, we need to pass the Keras model to the first argument of the AutoQKeras class to create an AutoQKeras object. The input Keras model is automatically converted to a QKeras model during the building process by the ```quantize_model()``` method of AutoQKHyperModel class (see [autoqkeras_internal.py#L570](https://github.com/google/qkeras/blob/1ab354276a041b45cd72c300e89a7c51ec99fa35/qkeras/autoqkeras/autoqkeras_internal.py#L570)). \n",
    "\n",
    "The idea is to exploit this automatic convertion to realize a QKeras model according to the methodology previously described in this notebook ([Ch. 1](#ch1)) with as minimum changes as possible to the original Keras model definition, as explained here:\n",
    "\n",
    "1) Every 2DConv, DWConv and FC layer has to be anticipated by an ```Activation``` layer with whatever activation function (we don't care about the type of activation because it will be replaced by AutoQKeras during the search with the ```activation``` values defined in the search space ```quantization_config```). In this example we are going to use \"sigmoid\", but any other type would be fine;\n",
    "\n",
    "2) The last 2DConv, DWConv or FC layer of the network has also to be followed by an ```Activation``` layer, as written in the previous point;\n",
    "\n",
    "3) Every ```BatchNormalization``` layer that follows a 2DConv or DWConv layer has to be fused with the convolution. We can use the flag ```enable_bn_folding=True``` when instantiating the AutoQKeras object to automatically do the batch normalization fusion;\n",
    "\n",
    "4) Every activation layer associated to a convolutional or fully-connected layer, either declared as argument of the ```Activation``` layer (such as ```Activation(activation=\"relu\")```), or declared as argument of the ```Conv2D```, ```DepthwiseConv2D``` or ```Dense``` layer (such as ```Conv2D(activation=\"relu\")```), has to be written in the \"direct\" form, i.e. using a layer that has the same name of the activation (such as ```ReLU()```);\n",
    "\n",
    "5) The activations not associated to a convolutional or fully-connected layer, usually those placed as last layer of CNNs (such as softmax), has not to be changed;\n",
    "\n",
    "6) Any other layer, such as pooling or add layers, has not to be changed;\n",
    "\n",
    "7) After average pooling, since the average of integers is not necessarily an integer, a quantization step is required (it would be better to use the same quantizer used for its input because its output range does not significantly change). Moreover, the dynamic of the activations changes between input and output of pooling. So an ```Activation``` layer is required after an average pooling;\n",
    "\n",
    "8) For residual connections (\"add\" layers), do as the next cell shows.\n",
    "\n",
    "Things to know:\n",
    "- ```quantize_model()``` does not accept an input model with QKeras layers\n",
    "\n",
    "To run the next cells without issues, you should edit the file [autoqkeras/autoqkeras_internal.py](https://github.com/google/qkeras/blob/master/qkeras/autoqkeras/autoqkeras_internal.py) to expose the flag ```enable_bn_folding``` to the AutoQKeras interface (externally) and to connect it to the ```AutoQKHyperModel``` class and its ```model_quantize``` method (internally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Keras model for AutoQKeras\n",
    "model2 = tf.keras.models.Sequential([\n",
    "    \n",
    "    MaxPooling2D(pool_size=pool_size_list[0], name=\"pool_0\"),\n",
    "    Activation(\"sigmoid\", name=\"act_0\"), # fake relu\n",
    "    \n",
    "    # Esempio conv senza batchnorm\n",
    "    Conv2D(filters=filters_list[0], kernel_size=kernel_size_list[0], strides=strides_list[0], padding=pads_list[0], name=\"conv2d_0\"),\n",
    "          #activation=\"relu\"),\n",
    "    ReLU(name=\"relu_0\"), # real relu\n",
    "    Activation(\"sigmoid\", name=\"act_1\"), # fake relu\n",
    "    \n",
    "    # Esempio conv con batchnorm\n",
    "    Conv2D(filters_list[1], kernel_size_list[1], strides_list[1], pads_list[1], name=\"conv2d_1\"),\n",
    "    BatchNormalization(name=\"bn_1\"),\n",
    "    ReLU(name=\"relu_1\"), # real relu\n",
    "    Activation(\"sigmoid\", name=\"act_2\"), # fake relu\n",
    "    \n",
    "    Flatten(name=\"flatten\"),\n",
    "    \n",
    "    # Esempio dense senza batchnorm\n",
    "    Dense(units_list[0], name=\"dense\"),\n",
    "    Activation(\"sigmoid\", name=\"act_3\"), # fake relu\n",
    "  \n",
    "    Activation(\"softmax\", name=\"softmax\")\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pool_0 (MaxPooling2D)        (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "act_0 (Activation)           (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_0 (Conv2D)            (None, 5, 5, 2)           20        \n",
      "_________________________________________________________________\n",
      "relu_0 (ReLU)                (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "act_1 (Activation)           (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 3, 3)           57        \n",
      "_________________________________________________________________\n",
      "bn_1 (BatchNormalization)    (None, 3, 3, 3)           12        \n",
      "_________________________________________________________________\n",
      "relu_1 (ReLU)                (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "act_2 (Activation)           (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                280       \n",
      "_________________________________________________________________\n",
      "act_3 (Activation)           (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 369\n",
      "Trainable params: 363\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.build((None,input_width,input_width,input_channels))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n"
     ]
    }
   ],
   "source": [
    "model2.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'],\n",
    "               run_eagerly=True)\n",
    "\n",
    "if tf.config.list_physical_devices('GPU') == []:\n",
    "    print(\"No GPU available\")\n",
    "else:\n",
    "    print(\"GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/lucaurbinati/opt/miniconda3/envs/qkeras-env/lib/python3.8/site-packages/qkeras/qtools/qgraph.py:189: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "operation count for <tensorflow.python.keras.layers.advanced_activations.ReLU object at 0x15e8f40d0> is defaulted to 0\n",
      "operation count for <tensorflow.python.keras.layers.advanced_activations.ReLU object at 0x15e89e0d0> is defaulted to 0\n",
      "Limit configuration:{\"Dense\": [16, 31, 16], \"Conv2D\": [16, 31, 16], \"DepthwiseConv2D\": [16, 31, 16], \"Activation\": [16], \"BatchNormalization\": []}\n",
      "operation count for <tensorflow.python.keras.layers.advanced_activations.ReLU object at 0x161542670> is defaulted to 0\n",
      "operation count for <tensorflow.python.keras.layers.advanced_activations.ReLU object at 0x16155e760> is defaulted to 0\n",
      "learning_rate: 0.0010000000474974513\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pool_0_input (InputLayer)    [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "pool_0 (MaxPooling2D)        (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "act_0 (QActivation)          (None, 7, 7, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_0 (QConv2D)           (None, 5, 5, 2)           20        \n",
      "_________________________________________________________________\n",
      "relu_0 (ReLU)                (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "act_1 (QActivation)          (None, 5, 5, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (QConv2DBatchnorm)  (None, 3, 3, 3)           70        \n",
      "_________________________________________________________________\n",
      "relu_1 (ReLU)                (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "act_2 (QActivation)          (None, 3, 3, 3)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 27)                0         \n",
      "_________________________________________________________________\n",
      "dense (QDense)               (None, 10)                280       \n",
      "_________________________________________________________________\n",
      "act_3 (QActivation)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 370\n",
      "Trainable params: 363\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n",
      "stats: delta_p=0.05 delta_n=0.05 rate=2.0 trial_size=90 reference_size=587\n",
      "       delta=13.53%\n",
      "Total Cost Reduction:\n",
      "       90 vs 587 (-84.67%)\n",
      "act_0                quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "conv2d_0             f=2 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_1                quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "conv2d_1             f=3 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_2                quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "dense                u=10 quantized_bits(4,4,1,alpha='auto') quantized_bits(16,16,1,alpha='auto') \n",
      "act_3                quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\n",
      "\n",
      "Search space summary\n",
      "Default search space size: 10\n",
      "conv2d_0_kernel_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto')\", \"quantized_bits(8,8,1,1,alpha='auto')\", \"quantized_bits(16,16,1,1,alpha='auto')\"], 'ordered': False}\n",
      "conv2d_1_kernel_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto')\", \"quantized_bits(8,8,1,1,alpha='auto')\", \"quantized_bits(16,16,1,1,alpha='auto')\"], 'ordered': False}\n",
      "dense_kernel_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto')\", \"quantized_bits(8,8,1,1,alpha='auto')\", \"quantized_bits(16,16,1,1,alpha='auto')\"], 'ordered': False}\n",
      "act_0_activation_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits(8,8,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits(16,16,1,1,alpha='auto',scale_axis=0)\"], 'ordered': False}\n",
      "conv2d_0_bias_quantizer (Choice)\n",
      "{'default': \"quantized_bits(16,16,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(16,16,1,1,alpha='auto')\", \"quantized_bits(31,31,1,1,alpha='auto')\"], 'ordered': False}\n",
      "act_1_activation_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits(8,8,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits(16,16,1,1,alpha='auto',scale_axis=0)\"], 'ordered': False}\n",
      "conv2d_1_bias_quantizer (Choice)\n",
      "{'default': \"quantized_bits(16,16,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(16,16,1,1,alpha='auto')\", \"quantized_bits(31,31,1,1,alpha='auto')\"], 'ordered': False}\n",
      "act_2_activation_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits(8,8,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits(16,16,1,1,alpha='auto',scale_axis=0)\"], 'ordered': False}\n",
      "dense_bias_quantizer (Choice)\n",
      "{'default': \"quantized_bits(16,16,1,1,alpha='auto')\", 'conditions': [], 'values': [\"quantized_bits(16,16,1,1,alpha='auto')\", \"quantized_bits(31,31,1,1,alpha='auto')\"], 'ordered': False}\n",
      "act_3_activation_quantizer (Choice)\n",
      "{'default': \"quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\", 'conditions': [], 'values': [\"quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits(8,8,1,1,alpha='auto',scale_axis=0)\", \"quantized_bits(16,16,1,1,alpha='auto',scale_axis=0)\"], 'ordered': False}\n"
     ]
    }
   ],
   "source": [
    "# scale_axis=0 does a per-layer quantization, i.e. one scaling factor for the entire layer\n",
    "# otherwise qkeras automatically does a per-channel quantization, i.e. one scaling facor for each channel in layer\n",
    "# we want a per-channel quantization for weigths/biases and a per-layer quantization for activations\n",
    "quantization_config = {\n",
    "        \"kernel\": {\n",
    "                \"quantized_bits(4,4,1,1,alpha='auto')\": 4,\n",
    "                \"quantized_bits(8,8,1,1,alpha='auto')\": 8,\n",
    "                \"quantized_bits(16,16,1,1,alpha='auto')\": 16,\n",
    "        },\n",
    "        \"bias\": {\n",
    "                \"quantized_bits(16,16,1,1,alpha='auto')\": 16,\n",
    "                \"quantized_bits(31,31,1,1,alpha='auto')\": 31,\n",
    "        },\n",
    "        \"activation\": {\n",
    "                \"quantized_bits(4,4,1,1,alpha='auto',scale_axis=0)\": 4,\n",
    "                \"quantized_bits(8,8,1,1,alpha='auto',scale_axis=0)\": 8,\n",
    "                \"quantized_bits(16,16,1,1,alpha='auto',scale_axis=0)\": 16\n",
    "        }\n",
    "}\n",
    "\n",
    "# w, b, a\n",
    "limit = {\n",
    "    \"Dense\": [16, 31, 16],\n",
    "    \"Conv2D\": [16, 31, 16],\n",
    "    \"DepthwiseConv2D\": [16, 31, 16],\n",
    "    \"Activation\": [16],\n",
    "    \"BatchNormalization\": []\n",
    "}\n",
    "\n",
    "goal = {\n",
    "    \"type\": \"energy\",\n",
    "    \"params\": {\n",
    "        \"delta_p\": 5.0,\n",
    "        \"delta_n\": 5.0,\n",
    "        \"rate\": 2.0,\n",
    "        \"stress\": 1.0,\n",
    "        \"process\": \"horowitz\",\n",
    "        \"parameters_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"activations_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"rd_wr_on_io\": [False, False],\n",
    "        \"min_sram_size\": [0, 0],\n",
    "        \"source_quantizers\": [\"int8\"],\n",
    "        \"reference_internal\": \"int8\",\n",
    "        \"reference_accumulator\": \"int16\"\n",
    "        }\n",
    "}\n",
    "\n",
    "run_config = {\n",
    "  \"output_dir\": \"./autoqkeras\",\n",
    "  \"goal\": goal,\n",
    "  \"quantization_config\": quantization_config,\n",
    "  \"learning_rate_optimizer\": False,\n",
    "  \"transfer_weights\": False,\n",
    "  \"mode\": \"random\",\n",
    "  \"seed\": 42,\n",
    "  \"limit\": limit,\n",
    "  \"tune_filters\": \"none\",\n",
    "  \"tune_filters_exceptions\": \"^dense\",\n",
    "  \"distribution_strategy\": tf.distribute.get_strategy(),\n",
    "  # first layer is input, layer two layers are softmax and flatten\n",
    "  \"layer_indexes\": range(1, len(model.layers) - 1),\n",
    "  \"max_trials\": 5\n",
    "}\n",
    "\n",
    "from qkeras.autoqkeras import *\n",
    "\n",
    "autoqk = AutoQKeras(model2, metrics=[\"acc\"], custom_objects={}, **run_config, enable_bn_folding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 06s]\n",
      "val_score: 0.3983521230307274\n",
      "\n",
      "Best val_score So Far: 0.5100576839010837\n",
      "Total elapsed time: 00h 00m 29s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "autoqk.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=1024, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_calibration_data(model,x_test,test_type):        #see generate_from_python\n",
    "\n",
    "    tot_iter = x_test.shape[0]\n",
    "\n",
    "    # Reset all lists\n",
    "\n",
    "    of_list = []\n",
    "    alphaq_of_list = []\n",
    "    betaq_of_list = []\n",
    "    fq_list = []\n",
    "    wq_list = []\n",
    "    bq_list = []\n",
    "    zeropoint_of_list = []\n",
    "    scale_of_list = []\n",
    "    scale_w_list = []\n",
    "    scale_b_list = []\n",
    "    subq1_list = []\n",
    "    layer_index = 0\n",
    "    ready1 = 0\n",
    "    previous_layers = [\"None\", \"None\",\"None\"]\n",
    "\n",
    "\n",
    "    if (test_type!=\"resnet\"):\n",
    "        layers_list = [layer for layer in model.layers]\n",
    "    else:   #resnet\n",
    "        layers_names_list = [\"input_1\",\"activation\",\"conv2d\",\"re_lu\",                                                           #top\n",
    "                             \"activation_1\",\"conv2d_1\",\"re_lu_1\",\"activation_2\",\"conv2d_2\",\"activation_3\",                      #first stack RIGHT, LEFT EMPTY\n",
    "                             \"add\",                                                                                             \n",
    "                             \"re_lu_2\",\"activation_4\",\"conv2d_3\",\"re_lu_3\",\"activation_5\",\"conv2d_4\",\"activation_6\",            #second stack RIGHT\n",
    "                             \"activation_7\",\"conv2d_5\",\"activation_8\",                                                          #second stack LEFT\n",
    "                             \"add_1\",   \n",
    "                             \"re_lu_4\", \"activation_9\",\"conv2d_6\",\"re_lu_5\",\"activation_10\",\"conv2d_7\",\"activation_11\",         #third stack RIGHT\n",
    "                             \"activation_12\",\"conv2d_8\",\"activation_13\",                                                        #third stack LEFT\n",
    "                             \"add_2\",                                                                                           \n",
    "                             \"re_lu_6\",\"average_pooling2d\",\"activation_14\",\"flatten\",\"dense\",\"activation_15\",\"softmax\"]         #bottom\n",
    "        \n",
    "        layers_list = []\n",
    "        for name in layers_names_list:\n",
    "            layers_list.append(model.get_layer(name))\n",
    "\n",
    "\n",
    "    ##############################CALIBRATION####################################\n",
    "    ##############################################################################\n",
    "    \"\"\"Two separate dictionaries for weight, bias and activations\n",
    "       1) weight and bias: scale and zero are known and fixed (zero is actually 0)   ---> extract them by accessing the quantizer attributes\n",
    "       2) activations: scale and bias vary with the input tensor                     ---> extract them by appending the output feature and taking max(|of|), alphaq,betaq instead are fixed by the quantizer\n",
    "\n",
    "       merge them at the end of calibration after extracting alpha and beta for the act dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    base_param_dict = {\n",
    "               \"w_scale\": 0,\n",
    "               \"b_scale\": 0,\n",
    "               \"SF_IN_W\": [],\n",
    "               \"WEIGHTS_CROSSPRODUCT\": []\n",
    "    }\n",
    "\n",
    "    base_act_dict = {\n",
    "        \"alphaq\": 0,\n",
    "        \"betaq\": 0,\n",
    "        \"alpha\": [],\n",
    "        \"beta\": [],\n",
    "        \"scale\": [],\n",
    "        \"SF_OUT_INV\": [],\n",
    "        \"zero(Z_IN)\":[]\n",
    "    }\n",
    "\n",
    "\n",
    "    #########DEFINE THE WEIGHT AND BIAS DICTIONARY###########################\n",
    "    w_layers = []\n",
    "    for layer in layers_list:\n",
    "        if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\", \"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\", \"QDense\", \"QDenseBatchnorm\"]:\n",
    "            w_layers.append(layer.name)\n",
    "\n",
    "    w_dict = {k: dc(base_param_dict) for k in w_layers} #deepcopy base dict otherwise it is always the same object\n",
    "\n",
    "    ########DEFINE THE ACTIVATION DICTIONARY########################\n",
    "    a_layers = []\n",
    "    for layer in layers_list:\n",
    "        if layer.__class__.__name__ in [\"QActivation\"]:\n",
    "            a_layers.append(layer.name)\n",
    "    \n",
    "    #inner_a_dict = {k: [] for k in base_act_keys}\n",
    "    a_dict = {k: dc(base_act_dict) for k in a_layers}   \n",
    "\n",
    "    for iter, x in enumerate(x_test):\n",
    "\n",
    "        if (test_type != \"auto-encoder\"):\n",
    "            data = x.reshape(1, x.shape[0], x.shape[1], x.shape[2])\n",
    "        else:\n",
    "            data = x.reshape(1,640)\n",
    "\n",
    "        of_list.append(data)\n",
    "        layer_index += 1\n",
    "\n",
    "        pred_model = np.asarray(model.predict(data, batch_size=1, verbose=0), dtype=np.float64)\n",
    "\n",
    "        ############LAYERS LOOP##############################\n",
    "\n",
    "        for layer in layers_list:\n",
    "            extractor = tf.keras.Model( inputs=model.inputs,\n",
    "                                       outputs=model.get_layer(layer.name).output)\n",
    "            of = extractor(data).numpy()\n",
    "            of_list.append(of)\n",
    "\n",
    "            ##### WEIGHTS #####\n",
    "            if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\", \"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\", \"QDense\", \"QDenseBatchnorm\"]:\n",
    "\n",
    "                if layer.__class__.__name__ in [\"QConv2D\", \"QDepthwiseConv2D\", \"QDense\"]:\n",
    "                    parameters = layer.weights\n",
    "                else:\n",
    "                    parameters = layer.get_folded_weights() # folded weights not quantized\n",
    "\n",
    "                w = parameters[0].numpy()\n",
    "                quantizer_w = layer.get_quantizers()[0]\n",
    "                alphaq_w = quantizer_w.alphaq\n",
    "                betaq_w = quantizer_w.betaq\n",
    "                scale1_w = quantizer_w.scale1.numpy().flatten()\n",
    "                m_i = quantizer_w.m_i.numpy().flatten()\n",
    "                scale_w = scale1_w * m_i                    #WEIGHT SCALE\n",
    "                scale_w_list.append(scale_w)\n",
    "\n",
    "                #QUANTIZED WEIGHTS\n",
    "                tmp = np.divide(w, scale_w, dtype=np.float64)\n",
    "                tmp[np.isnan(tmp)] = 0\n",
    "                tmp[np.isinf(tmp)] = 0\n",
    "                wq = np.clip(np.trunc(tmp + np.sign(tmp)*0.5), alphaq_w, betaq_w)\n",
    "                wq_list.append(wq)\n",
    "\n",
    "    \n",
    "                # print \"subq1\" (part 2)\n",
    "                if layer.__class__.__name__ in [\"QConv2D\", \"QConv2DBatchnorm\"]:\n",
    "                    sum_of_weights = wq.sum(axis=(0, 1, 2))\n",
    "                elif layer.__class__.__name__ in [\"QDense\", \"QDenseBatchnorm\"]:\n",
    "                    sum_of_weights = wq.sum(axis=0)\n",
    "                if layer.__class__.__name__ in [\"QDepthwiseConv2D\", \"QDepthwiseConv2DBatchnorm\"]:\n",
    "                    sum_of_weights = wq.sum(axis=(0, 1))\n",
    "\n",
    "                if ready1 == 1:\n",
    "                    subq1 = (zeropoint_of_list[-1] * sum_of_weights).flatten()\n",
    "                    sx = float(scale_of_list[-1])\n",
    "                    sw = float(np.max(np.abs(scale_w)))\n",
    "                    sx_sw = sx*sw\n",
    "                    subq1_list.append(subq1)\n",
    "                    ready1 = 0\n",
    "                    w_dict[layer.name][\"SF_IN_W\"].append(sx_sw)\n",
    "                    w_dict[layer.name][\"WEIGHTS_CROSSPRODUCT\"].append(np.max(np.abs(subq1)))\n",
    "                   \n",
    "\n",
    "                ##### BIASES ######\n",
    "                quantizer_b = layer.get_quantizers()[1]\n",
    "                b = parameters[1].numpy()\n",
    "                alphaq_b = quantizer_b.alphaq\n",
    "                betaq_b = quantizer_b.betaq\n",
    "                scale1_b = quantizer_b.scale1.numpy().flatten()\n",
    "                m_i = quantizer_b.m_i.numpy().flatten()\n",
    "                scale_b = scale1_b * m_i\n",
    "\n",
    "\n",
    "                scale_b_list.append(scale_b)\n",
    "                if scale_b != 0:\n",
    "                    tmp = np.divide(b, scale_b, dtype=np.float64)\n",
    "                    tmp[np.isnan(tmp)] = 0\n",
    "                    tmp[np.isinf(tmp)] = 0\n",
    "                    bq = np.clip(np.trunc(tmp + np.sign(tmp)*0.5), alphaq_b, betaq_b)\n",
    "                else:\n",
    "                    bq = np.zeros(b.size)\n",
    "                bq_list.append(bq)\n",
    "\n",
    "\n",
    "                #calibration, all constants x layer\n",
    "                w_dict[layer.name]['w_scale'] = np.max(np.abs(scale_w))\n",
    "                w_dict[layer.name][\"b_scale\"] = np.max(np.abs(scale_b))                \n",
    "                w_dict[layer.name][\"BIASQ_SCALE\"] = np.max(np.abs(bq*scale_b))\n",
    "\n",
    "\n",
    "            ##### FEATURES #####\n",
    "            elif layer.__class__.__name__ in [\"QActivation\"]:\n",
    "\n",
    "                # axis 0 is for batch, 1 and 2 are for feature map, 3 is for channels\n",
    "                quantizer_of = layer.quantizer\n",
    "                alphaq_of = quantizer_of.alphaq\n",
    "                betaq_of = quantizer_of.betaq\n",
    "\n",
    "                #calcola min e max invece di of\n",
    "                alpha_of = quantizer_of.alpha_f.numpy().flatten()\n",
    "                beta_of = quantizer_of.beta_f.numpy().flatten()\n",
    "\n",
    "\n",
    "                alphaq_of_list.append(alphaq_of)\n",
    "                betaq_of_list.append(betaq_of)\n",
    "\n",
    "                scale1_of = quantizer_of.scale1.numpy().flatten()\n",
    "                scale_of2 = scale1_of\n",
    "                scale_of_list.append(scale_of2)\n",
    "\n",
    "                z_of = quantizer_of.zeropoint.numpy().flatten()\n",
    "                z_of[np.isnan(z_of)] = 0\n",
    "                z_of[np.isinf(z_of)] = 0\n",
    "                zeropoint_of_list.append(z_of)\n",
    "\n",
    "                ready1 = 1 # print \"subq1\" (part 1)\n",
    "                ofq = quantizer_of.outputq.numpy()\n",
    "                fq_list.append(ofq)\n",
    "\n",
    "                #calibration\n",
    "                a_dict[layer.name][\"alphaq\"] = alphaq_of\n",
    "                a_dict[layer.name][\"betaq\"] = betaq_of\n",
    "                a_dict[layer.name][\"alpha\"].append(alpha_of[0])   #[0] because all these are arrays with a single value\n",
    "                a_dict[layer.name][\"beta\"].append(beta_of[0])\n",
    "                a_dict[layer.name][\"scale\"].append(scale_of2[0])\n",
    "                a_dict[layer.name][\"SF_OUT_INV\"].append(1/scale_of2[0])\n",
    "                a_dict[layer.name][\"zero(Z_IN)\"].append(z_of[0])\n",
    "\n",
    "            layer_index += 1\n",
    "            previous_layers.append(layer.__class__.__name__)\n",
    "        print(f\"ITERATION: {iter+1}/{tot_iter}\")\n",
    "    return (w_dict, a_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv(w_dict,a_dict,csv_file_path1,csv_file_path2):\n",
    "    df = pd.DataFrame.from_dict(w_dict, orient=\"index\") #index will be determined by the first layer of nested dictionaries (layers)\n",
    "    df.to_csv(csv_file_path1)\n",
    "    df = pd.DataFrame.from_dict(a_dict, orient=\"index\") #index will be determined by the first layer of nested dictionaries (layers)\n",
    "    df.to_csv(csv_file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mobilenet_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (x_test,y_test) \u001b[38;5;241m=\u001b[39m \u001b[43mmobilenet_utils\u001b[49m\u001b[38;5;241m.\u001b[39mget_mobilenet_data() \n\u001b[1;32m      2\u001b[0m qmodel \u001b[38;5;241m=\u001b[39m qkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mload_qmodel(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[1;32m      3\u001b[0m qmodel\u001b[38;5;241m.\u001b[39mcompile(Adam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m], metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m                run_eagerly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mobilenet_utils' is not defined"
     ]
    }
   ],
   "source": [
    "(x_test,y_test) = mobilenet_utils.get_mobilenet_data() \n",
    "qmodel = qkeras.utils.load_qmodel(...)\n",
    "qmodel.compile(Adam(lr=0.001), loss=['categorical_crossentropy'], metrics=['accuracy'],\n",
    "               run_eagerly=True)\n",
    "\n",
    "(w_dict,a_dict) = extract_calibration_data(qmodel, x_test)\n",
    "generate_csv(w_dict, a_dict, \"./calibration_results/extracted_weights.csv\", \"./calibration_results/extracted_activations.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
